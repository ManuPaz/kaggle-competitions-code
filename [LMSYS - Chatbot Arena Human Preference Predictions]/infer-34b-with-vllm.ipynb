{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7851bfe5",
   "metadata": {
    "papermill": {
     "duration": 0.006516,
     "end_time": "2024-07-18T10:13:20.271554",
     "exception": false,
     "start_time": "2024-07-18T10:13:20.265038",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Submit LLM 34B Model in 5 hours!\n",
    "This notebook demonstrates how to submit a LLM 34B model in only 5 hours! Amazing! The key tricks are:\n",
    "* use vLLM (for speed)\n",
    "* use AWQ 4bit quantization (to avoid GPU VRAM OOM)\n",
    "* limit input size to 1024 tokens (for speed)\n",
    "* limit output size to 1 token (for speed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413f8ad1",
   "metadata": {
    "papermill": {
     "duration": 0.005863,
     "end_time": "2024-07-18T10:13:20.283454",
     "exception": false,
     "start_time": "2024-07-18T10:13:20.277591",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pip Install vLLM\n",
    "The package vLLM is an incredibly fast LLM inference library! The vLLM that is installed in Kaggle notebooks will produce errors, therefore we need to reinstall vLLM. The code below was taken from notebook [here][1]\n",
    "\n",
    "[1]: https://www.kaggle.com/code/lewtun/numina-1st-place-solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a9d31ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:13:20.296336Z",
     "iopub.status.busy": "2024-07-18T10:13:20.296050Z",
     "iopub.status.idle": "2024-07-18T10:13:20.306847Z",
     "shell.execute_reply": "2024-07-18T10:13:20.305612Z"
    },
    "papermill": {
     "duration": 0.019553,
     "end_time": "2024-07-18T10:13:20.308935",
     "exception": false,
     "start_time": "2024-07-18T10:13:20.289382",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, math, numpy as np\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e11154b",
   "metadata": {
    "_kg_hide-output": true,
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-07-18T10:13:20.321760Z",
     "iopub.status.busy": "2024-07-18T10:13:20.321496Z",
     "iopub.status.idle": "2024-07-18T10:16:35.214755Z",
     "shell.execute_reply": "2024-07-18T10:16:35.213647Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 194.913428,
     "end_time": "2024-07-18T10:16:35.228252",
     "exception": false,
     "start_time": "2024-07-18T10:13:20.314824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.1.2\r\n",
      "Uninstalling torch-2.1.2:\r\n",
      "  Successfully uninstalled torch-2.1.2\r\n",
      "Looking in links: /kaggle/input/vllm-whl\r\n",
      "Processing /kaggle/input/vllm-whl/vllm-0.4.0.post1-cp310-cp310-manylinux1_x86_64.whl\r\n",
      "Processing /kaggle/input/vllm-whl/cmake-3.29.0.1-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\r\n",
      "Requirement already satisfied: ninja in /opt/conda/lib/python3.10/site-packages (from vllm) (1.11.1.1)\r\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from vllm) (5.9.3)\r\n",
      "Requirement already satisfied: ray>=2.9 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.9.0)\r\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from vllm) (0.2.0)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from vllm) (1.26.4)\r\n",
      "Processing /kaggle/input/vllm-whl/torch-2.1.2-cp310-cp310-manylinux1_x86_64.whl (from vllm)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from vllm) (2.32.3)\r\n",
      "Requirement already satisfied: py-cpuinfo in /opt/conda/lib/python3.10/site-packages (from vllm) (9.0.0)\r\n",
      "Requirement already satisfied: transformers>=4.39.1 in /opt/conda/lib/python3.10/site-packages (from vllm) (4.42.3)\r\n",
      "Processing /kaggle/input/vllm-whl/xformers-0.0.23.post1-cp310-cp310-manylinux2014_x86_64.whl (from vllm)\r\n",
      "Requirement already satisfied: fastapi in /opt/conda/lib/python3.10/site-packages (from vllm) (0.108.0)\r\n",
      "Requirement already satisfied: uvicorn[standard] in /opt/conda/lib/python3.10/site-packages (from vllm) (0.25.0)\r\n",
      "Requirement already satisfied: pydantic>=2.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (2.5.3)\r\n",
      "Requirement already satisfied: prometheus-client>=0.18.0 in /opt/conda/lib/python3.10/site-packages (from vllm) (0.19.0)\r\n",
      "Processing /kaggle/input/vllm-whl/pynvml-11.5.0-py3-none-any.whl (from vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/outlines-0.0.34-py3-none-any.whl (from vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (from vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/interegular-0.3.3-py37-none-any.whl (from outlines==0.0.34->vllm)\r\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (3.1.2)\r\n",
      "Processing /kaggle/input/vllm-whl/lark-1.1.9-py3-none-any.whl (from outlines==0.0.34->vllm)\r\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.5.8)\r\n",
      "Requirement already satisfied: cloudpickle in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (2.2.1)\r\n",
      "Processing /kaggle/input/vllm-whl/diskcache-5.6.3-py3-none-any.whl (from outlines==0.0.34->vllm)\r\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.11.4)\r\n",
      "Requirement already satisfied: numba in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (0.58.1)\r\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (1.4.2)\r\n",
      "Requirement already satisfied: referencing in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (0.32.1)\r\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from outlines==0.0.34->vllm) (4.20.0)\r\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken==0.6.0->vllm) (2023.12.25)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (3.13.1)\r\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (4.9.0)\r\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (1.13.0)\r\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (3.2.1)\r\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.1.2->vllm) (2024.5.0)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (from torch==2.1.2->vllm)\r\n",
      "Processing /kaggle/input/vllm-whl/nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.2->vllm)\r\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->vllm) (0.6.0)\r\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2.0->vllm) (2.14.6)\r\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (8.1.7)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.0.7)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (3.20.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (6.0.1)\r\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray>=2.9->vllm) (1.4.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->vllm) (2024.7.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.23.4)\r\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.4.3)\r\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (0.19.1)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.39.1->vllm) (4.66.4)\r\n",
      "Requirement already satisfied: starlette<0.33.0,>=0.29.0 in /opt/conda/lib/python3.10/site-packages (from fastapi->vllm) (0.32.0.post1)\r\n",
      "Requirement already satisfied: h11>=0.8 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.14.0)\r\n",
      "Requirement already satisfied: httptools>=0.5.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.6.1)\r\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (1.0.0)\r\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.19.0)\r\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (0.21.0)\r\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/conda/lib/python3.10/site-packages (from uvicorn[standard]->vllm) (12.0)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray>=2.9->vllm) (3.1.1)\r\n",
      "Requirement already satisfied: anyio<5,>=3.4.0 in /opt/conda/lib/python3.10/site-packages (from starlette<0.33.0,>=0.29.0->fastapi->vllm) (4.2.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->outlines==0.0.34->vllm) (2.1.3)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (23.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (2023.12.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->outlines==0.0.34->vllm) (0.16.2)\r\n",
      "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /opt/conda/lib/python3.10/site-packages (from numba->outlines==0.0.34->vllm) (0.41.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.1.2->vllm) (1.3.0)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi->vllm) (1.3.0)\r\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio<5,>=3.4.0->starlette<0.33.0,>=0.29.0->fastapi->vllm) (1.2.0)\r\n",
      "Installing collected packages: triton, pynvml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lark, interegular, diskcache, cmake, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, xformers, outlines, vllm\r\n",
      "  Attempting uninstall: pynvml\r\n",
      "    Found existing installation: pynvml 11.4.1\r\n",
      "    Uninstalling pynvml-11.4.1:\r\n",
      "      Successfully uninstalled pynvml-11.4.1\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "ucx-py 0.38.0 requires libucx<1.16,>=1.15.0, which is not installed.\r\n",
      "ucxx 0.38.0 requires libucx>=1.15.0, which is not installed.\r\n",
      "dask-cuda 24.6.0 requires pynvml<11.5,>=11.0.0, but you have pynvml 11.5.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed cmake-3.29.0.1 diskcache-5.6.3 interegular-0.3.3 lark-1.1.9 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105 outlines-0.0.34 pynvml-11.5.0 tiktoken-0.6.0 torch-2.1.2 triton-2.1.0 vllm-0.4.0.post1 xformers-0.0.23.post1\r\n",
      "Processing /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\r\n",
      "Installing collected packages: grpcio\r\n",
      "  Attempting uninstall: grpcio\r\n",
      "    Found existing installation: grpcio 1.60.0\r\n",
      "    Uninstalling grpcio-1.60.0:\r\n",
      "      Successfully uninstalled grpcio-1.60.0\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "tensorflow-decision-forests 1.8.1 requires wurlitzer, which is not installed.\r\n",
      "apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.8 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.4 which is incompatible.\r\n",
      "apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 16.1.0 which is incompatible.\r\n",
      "tensorflow 2.15.0 requires keras<2.16,>=2.15.0, but you have keras 3.4.1 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed grpcio-1.62.2\r\n",
      "Processing /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl\r\n",
      "Requirement already satisfied: click>=7.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (8.1.7)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (3.13.1)\r\n",
      "Requirement already satisfied: jsonschema in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (4.20.0)\r\n",
      "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.0.7)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (21.3)\r\n",
      "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (3.20.3)\r\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (6.0.1)\r\n",
      "Requirement already satisfied: aiosignal in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.3.1)\r\n",
      "Requirement already satisfied: frozenlist in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (1.4.1)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from ray==2.11.0) (2.32.3)\r\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (23.2.0)\r\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (2023.12.1)\r\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (0.32.1)\r\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/conda/lib/python3.10/site-packages (from jsonschema->ray==2.11.0) (0.16.2)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->ray==2.11.0) (3.1.1)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (3.3.2)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (3.6)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (1.26.18)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->ray==2.11.0) (2024.7.4)\r\n",
      "Installing collected packages: ray\r\n",
      "  Attempting uninstall: ray\r\n",
      "    Found existing installation: ray 2.9.0\r\n",
      "    Uninstalling ray-2.9.0:\r\n",
      "      Successfully uninstalled ray-2.9.0\r\n",
      "Successfully installed ray-2.11.0\r\n",
      "CPU times: user 2.08 s, sys: 453 ms, total: 2.53 s\n",
      "Wall time: 3min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!pip uninstall -y torch\n",
    "!pip install -U --no-index --find-links=/kaggle/input/vllm-whl -U vllm\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/grpcio-1.62.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n",
    "!pip install -U --upgrade /kaggle/input/vllm-t4-fix/ray-2.11.0-cp310-cp310-manylinux2014_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da786691",
   "metadata": {
    "papermill": {
     "duration": 0.01025,
     "end_time": "2024-07-18T10:16:35.249127",
     "exception": false,
     "start_time": "2024-07-18T10:16:35.238877",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load 34B Quantized Model with vLLM!\n",
    "We will load and use LLM 34B Bagel [here][1]. This is a strong model.\n",
    "\n",
    "[1]: https://huggingface.co/jondurbin/bagel-34b-v0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca14135b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:16:35.271669Z",
     "iopub.status.busy": "2024-07-18T10:16:35.271357Z",
     "iopub.status.idle": "2024-07-18T10:19:10.549025Z",
     "shell.execute_reply": "2024-07-18T10:19:10.545336Z"
    },
    "papermill": {
     "duration": 155.295602,
     "end_time": "2024-07-18T10:19:10.555218",
     "exception": false,
     "start_time": "2024-07-18T10:16:35.259616",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 10:16:38,718\tINFO util.py:124 -- Outdated packages:\n",
      "  ipywidgets==7.7.1 found, needs ipywidgets>=8\n",
      "Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:16:38 config.py:211] awq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 10:16:41,358\tINFO worker.py:1749 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 07-18 10:16:43 llm_engine.py:74] Initializing an LLM engine (v0.4.0.post1) with config: model='/kaggle/input/bagel-v3-343', tokenizer='/kaggle/input/bagel-v3-343', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1024, download_dir=None, load_format=auto, tensor_parallel_size=2, disable_custom_all_reduce=True, quantization=awq, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, seed=0)\n",
      "INFO 07-18 10:16:50 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "INFO 07-18 10:16:50 selector.py:25] Using XFormers backend.\n",
      "\u001b[36m(RayWorkerVllm pid=355)\u001b[0m INFO 07-18 10:16:51 selector.py:40] Cannot use FlashAttention backend for Volta and Turing GPUs.\n",
      "\u001b[36m(RayWorkerVllm pid=355)\u001b[0m INFO 07-18 10:16:51 selector.py:25] Using XFormers backend.\n",
      "INFO 07-18 10:16:52 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "\u001b[36m(RayWorkerVllm pid=355)\u001b[0m INFO 07-18 10:16:52 pynccl_utils.py:45] vLLM is using nccl==2.18.1\n",
      "INFO 07-18 10:18:59 model_runner.py:104] Loading model weights took 9.0522 GB\n",
      "\u001b[36m(RayWorkerVllm pid=355)\u001b[0m INFO 07-18 10:18:59 model_runner.py:104] Loading model weights took 9.0522 GB\n",
      "INFO 07-18 10:19:04 ray_gpu_executor.py:240] # GPU blocks: 1810, # CPU blocks: 2184\n"
     ]
    }
   ],
   "source": [
    "import vllm\n",
    "\n",
    "llm = vllm.LLM(\n",
    "    \"/kaggle/input/bagel-v3-343\",\n",
    "    quantization=\"awq\",\n",
    "    tensor_parallel_size=2, \n",
    "    gpu_memory_utilization=0.95, \n",
    "    trust_remote_code=True,\n",
    "    dtype=\"half\", \n",
    "    enforce_eager=True,\n",
    "    max_model_len=1024,\n",
    "    #distributed_executor_backend=\"ray\",\n",
    ")\n",
    "tokenizer = llm.get_tokenizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba5562c",
   "metadata": {
    "papermill": {
     "duration": 0.012111,
     "end_time": "2024-07-18T10:19:10.580758",
     "exception": false,
     "start_time": "2024-07-18T10:19:10.568647",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Load Test Data\n",
    "During **commit** we load 128 rows of train to compute CV score. During **submit**, we load the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02229a1b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:19:10.609094Z",
     "iopub.status.busy": "2024-07-18T10:19:10.607065Z",
     "iopub.status.idle": "2024-07-18T10:19:17.581214Z",
     "shell.execute_reply": "2024-07-18T10:19:17.579216Z"
    },
    "papermill": {
     "duration": 6.992384,
     "end_time": "2024-07-18T10:19:17.584798",
     "exception": false,
     "start_time": "2024-07-18T10:19:10.592414",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>30192</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>[\"Is it morally right to try to have a certain...</td>\n",
       "      <td>[\"The question of whether it is morally right ...</td>\n",
       "      <td>[\"As an AI, I don't have personal beliefs or o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id             model_a     model_b  \\\n",
       "0  30192  gpt-4-1106-preview  gpt-4-0613   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"Is it morally right to try to have a certain...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"The question of whether it is morally right ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"As an AI, I don't have personal beliefs or o...               1   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               0           0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "VALIDATE = 128\n",
    "\n",
    "test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/test.csv\") \n",
    "if len(test)==3:\n",
    "    test = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/train.csv\")\n",
    "    test = test.iloc[:VALIDATE]\n",
    "print( test.shape )\n",
    "test.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598f7438",
   "metadata": {
    "papermill": {
     "duration": 0.011696,
     "end_time": "2024-07-18T10:19:17.614042",
     "exception": false,
     "start_time": "2024-07-18T10:19:17.602346",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Engineer Prompt\n",
    "If we want to submit zero shot LLM, we need to experiment with different system prompts to improve CV score. If we finetune the model, then system is not as important because the model will learn from the targets what to do regardless of which system prompt we use.\n",
    "\n",
    "We use a logits processor to force the model to output the 3 tokens we are interested in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73e0e7fc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:19:17.642036Z",
     "iopub.status.busy": "2024-07-18T10:19:17.640662Z",
     "iopub.status.idle": "2024-07-18T10:19:17.671335Z",
     "shell.execute_reply": "2024-07-18T10:19:17.669661Z"
    },
    "papermill": {
     "duration": 0.048143,
     "end_time": "2024-07-18T10:19:17.673944",
     "exception": false,
     "start_time": "2024-07-18T10:19:17.625801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Force predictions to be tokens [59603, 59616, 45228] which are ['A', 'B', 'tie'].\n"
     ]
    }
   ],
   "source": [
    "from typing import Any, Dict, List\n",
    "from transformers import LogitsProcessor\n",
    "import torch\n",
    "\n",
    "choices = [\"A\",\"B\",\"tie\"]\n",
    "\n",
    "KEEP = []\n",
    "for x in choices:\n",
    "    c = tokenizer.encode(x,add_special_tokens=False)[0]\n",
    "    KEEP.append(c)\n",
    "print(f\"Force predictions to be tokens {KEEP} which are {choices}.\")\n",
    "\n",
    "class DigitLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self, tokenizer):\n",
    "        self.allowed_ids = KEEP\n",
    "        \n",
    "    def __call__(self, input_ids: List[int], scores: torch.Tensor) -> torch.Tensor:\n",
    "        scores[self.allowed_ids] += 100\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79da8707",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:19:17.700694Z",
     "iopub.status.busy": "2024-07-18T10:19:17.699819Z",
     "iopub.status.idle": "2024-07-18T10:19:17.705875Z",
     "shell.execute_reply": "2024-07-18T10:19:17.703930Z"
    },
    "papermill": {
     "duration": 0.02216,
     "end_time": "2024-07-18T10:19:17.708907",
     "exception": false,
     "start_time": "2024-07-18T10:19:17.686747",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sys_prompt = \"\"\"Please read the following prompt and two responses. Determine which response is better.\n",
    "If the responses are relatively the same, respond with 'tie'. Otherwise respond with 'A' or 'B' to indicate which is better.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c37d639",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:19:17.736800Z",
     "iopub.status.busy": "2024-07-18T10:19:17.735140Z",
     "iopub.status.idle": "2024-07-18T10:19:17.744453Z",
     "shell.execute_reply": "2024-07-18T10:19:17.742352Z"
    },
    "papermill": {
     "duration": 0.026735,
     "end_time": "2024-07-18T10:19:17.747545",
     "exception": false,
     "start_time": "2024-07-18T10:19:17.720810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SS = \"#\"*25 + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "586d0fb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:19:17.774513Z",
     "iopub.status.busy": "2024-07-18T10:19:17.773416Z",
     "iopub.status.idle": "2024-07-18T10:19:17.853897Z",
     "shell.execute_reply": "2024-07-18T10:19:17.852045Z"
    },
    "papermill": {
     "duration": 0.096466,
     "end_time": "2024-07-18T10:19:17.856420",
     "exception": false,
     "start_time": "2024-07-18T10:19:17.759954",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_prompts = []\n",
    "for index,row in test.iterrows():\n",
    "    \n",
    "    a = \" \".join(eval(row.prompt, {\"null\": \"\"}))\n",
    "    b = \" \".join(eval(row.response_a, {\"null\": \"\"}))\n",
    "    c = \" \".join(eval(row.response_b, {\"null\": \"\"}))\n",
    "    \n",
    "    prompt = f\"{SS}PROMPT: \"+a+f\"\\n\\n{SS}RESPONSE A: \"+b+f\"\\n\\n{SS}RESPONSE B: \"+c+\"\\n\\n\"\n",
    "    \n",
    "    formatted_sample = sys_prompt + \"\\n\\n\" + prompt\n",
    "    \n",
    "    all_prompts.append( formatted_sample )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833ff356",
   "metadata": {
    "papermill": {
     "duration": 0.011639,
     "end_time": "2024-07-18T10:19:17.880493",
     "exception": false,
     "start_time": "2024-07-18T10:19:17.868854",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Infer Test\n",
    "We infer test using fast vLLM. We ask vLLM to output probabilties of the top 5 tokens considered to be predicted in the first token. We also limit prediction to 1 token to increase inference speed.\n",
    "\n",
    "Based on the speed it takes to infer 128 train samples, we can deduce how long inferring 25,000 test samples will take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66077e3e",
   "metadata": {
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2024-07-18T10:19:17.907392Z",
     "iopub.status.busy": "2024-07-18T10:19:17.906671Z",
     "iopub.status.idle": "2024-07-18T10:20:56.668802Z",
     "shell.execute_reply": "2024-07-18T10:20:56.667712Z"
    },
    "papermill": {
     "duration": 98.778692,
     "end_time": "2024-07-18T10:20:56.671216",
     "exception": false,
     "start_time": "2024-07-18T10:19:17.892524",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   0%|          | 0/128 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:18 scheduler.py:245] Input prompt (1332 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:18 scheduler.py:245] Input prompt (1494 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:18 scheduler.py:245] Input prompt (1089 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   5%|▍         | 6/128 [00:06<01:59,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:24 scheduler.py:245] Input prompt (1033 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:25 scheduler.py:245] Input prompt (2317 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:   7%|▋         | 9/128 [00:09<01:58,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:28 scheduler.py:245] Input prompt (1102 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:28 scheduler.py:245] Input prompt (1977 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  16%|█▋        | 21/128 [00:16<01:16,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:35 scheduler.py:245] Input prompt (1176 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:35 scheduler.py:245] Input prompt (6926 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:35 scheduler.py:245] Input prompt (4419 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  20%|█▉        | 25/128 [00:19<01:14,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:38 scheduler.py:245] Input prompt (3084 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  25%|██▌       | 32/128 [00:23<01:01,  1.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:42 scheduler.py:245] Input prompt (1403 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:42 scheduler.py:245] Input prompt (3095 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  32%|███▏      | 41/128 [00:30<01:01,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:49 scheduler.py:245] Input prompt (1883 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  36%|███▌      | 46/128 [00:34<01:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:53 scheduler.py:245] Input prompt (1335 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:53 scheduler.py:245] Input prompt (1139 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:19:53 scheduler.py:245] Input prompt (1226 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  41%|████      | 52/128 [00:38<00:51,  1.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:19:56 scheduler.py:245] Input prompt (3367 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  48%|████▊     | 62/128 [00:45<00:46,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:04 scheduler.py:245] Input prompt (1178 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  50%|█████     | 64/128 [00:49<00:58,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:08 scheduler.py:245] Input prompt (1181 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  54%|█████▍    | 69/128 [00:53<00:51,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:12 scheduler.py:245] Input prompt (1557 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:20:12 scheduler.py:245] Input prompt (1365 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  59%|█████▊    | 75/128 [00:57<00:41,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:16 scheduler.py:245] Input prompt (1380 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:20:16 scheduler.py:245] Input prompt (1121 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  73%|███████▎  | 94/128 [01:13<00:31,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:31 scheduler.py:245] Input prompt (2022 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  76%|███████▌  | 97/128 [01:16<00:30,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:35 scheduler.py:245] Input prompt (1053 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  79%|███████▉  | 101/128 [01:20<00:27,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:39 scheduler.py:245] Input prompt (1044 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:20:39 scheduler.py:245] Input prompt (2196 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:20:39 scheduler.py:245] Input prompt (2288 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  83%|████████▎ | 106/128 [01:25<00:20,  1.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:43 scheduler.py:245] Input prompt (2541 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts:  91%|█████████▏| 117/128 [01:33<00:09,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 07-18 10:20:51 scheduler.py:245] Input prompt (1083 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:20:51 scheduler.py:245] Input prompt (1292 tokens) is too long and exceeds limit of 1024\n",
      "WARNING 07-18 10:20:51 scheduler.py:245] Input prompt (1077 tokens) is too long and exceeds limit of 1024\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 128/128 [01:38<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference of 128 samples took 1.645822032292684 minutes!\n",
      "CPU times: user 1min 38s, sys: 0 ns, total: 1min 38s\n",
      "Wall time: 1min 38s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from time import time\n",
    "start = time()\n",
    "\n",
    "logits_processors = [DigitLogitsProcessor(tokenizer)]\n",
    "responses = llm.generate(\n",
    "    all_prompts,\n",
    "    vllm.SamplingParams(\n",
    "        n=1,  # Number of output sequences to return for each prompt.\n",
    "        top_p=0.9,  # Float that controls the cumulative probability of the top tokens to consider.\n",
    "        temperature=0,  # randomness of the sampling\n",
    "        seed=777, # Seed for reprodicibility\n",
    "        skip_special_tokens=True,  # Whether to skip special tokens in the output.\n",
    "        max_tokens=1,  # Maximum number of tokens to generate per output sequence.\n",
    "        logits_processors=logits_processors,\n",
    "        logprobs = 5\n",
    "    ),\n",
    "    use_tqdm = True\n",
    ")\n",
    "\n",
    "end = time()\n",
    "elapsed = (end-start)/60. #minutes\n",
    "print(f\"Inference of {VALIDATE} samples took {elapsed} minutes!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bda9b75b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:20:56.706847Z",
     "iopub.status.busy": "2024-07-18T10:20:56.706568Z",
     "iopub.status.idle": "2024-07-18T10:20:56.711235Z",
     "shell.execute_reply": "2024-07-18T10:20:56.710321Z"
    },
    "papermill": {
     "duration": 0.024921,
     "end_time": "2024-07-18T10:20:56.713357",
     "exception": false,
     "start_time": "2024-07-18T10:20:56.688436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit will take 5.357493594702747 hours\n"
     ]
    }
   ],
   "source": [
    "submit = 25_000 / 128 * elapsed / 60\n",
    "print(f\"Submit will take {submit} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a377d021",
   "metadata": {
    "papermill": {
     "duration": 0.017379,
     "end_time": "2024-07-18T10:20:56.747744",
     "exception": false,
     "start_time": "2024-07-18T10:20:56.730365",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Extract Inference Probabilites\n",
    "We now extract the probabilties of \"A\", \"B\", \"tie\" from the vLLM predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98052385",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:20:56.782978Z",
     "iopub.status.busy": "2024-07-18T10:20:56.782723Z",
     "iopub.status.idle": "2024-07-18T10:20:56.793194Z",
     "shell.execute_reply": "2024-07-18T10:20:56.792344Z"
    },
    "papermill": {
     "duration": 0.030048,
     "end_time": "2024-07-18T10:20:56.795262",
     "exception": false,
     "start_time": "2024-07-18T10:20:56.765214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There were 33 inference errors out of 128 inferences\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "errors = 0\n",
    "\n",
    "for i,response in enumerate(responses):\n",
    "    try:\n",
    "        x = response.outputs[0].logprobs[0]\n",
    "        logprobs = []\n",
    "        for k in KEEP:\n",
    "            if k in x:\n",
    "                logprobs.append( math.exp(x[k].logprob) )\n",
    "            else:\n",
    "                logprobs.append( 0 )\n",
    "                print(f\"bad logits {i}\")\n",
    "        logprobs = np.array( logprobs )\n",
    "        logprobs /= logprobs.sum()\n",
    "        results.append( logprobs )\n",
    "    except:\n",
    "        #print(f\"error {i}\")\n",
    "        results.append( np.array([1/3., 1/3., 1/3.]) )\n",
    "        errors += 1\n",
    "        \n",
    "print(f\"There were {errors} inference errors out of {i+1} inferences\")\n",
    "results = np.vstack(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d63e757",
   "metadata": {
    "papermill": {
     "duration": 0.016682,
     "end_time": "2024-07-18T10:20:56.828857",
     "exception": false,
     "start_time": "2024-07-18T10:20:56.812175",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Create Submission CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fc05608",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:20:56.864716Z",
     "iopub.status.busy": "2024-07-18T10:20:56.864434Z",
     "iopub.status.idle": "2024-07-18T10:20:56.887904Z",
     "shell.execute_reply": "2024-07-18T10:20:56.887028Z"
    },
    "papermill": {
     "duration": 0.043472,
     "end_time": "2024-07-18T10:20:56.889833",
     "exception": false,
     "start_time": "2024-07-18T10:20:56.846361",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  winner_model_a  winner_model_b  winner_tie\n",
       "0   136060        0.333333        0.333333    0.333333\n",
       "1   211333        0.333333        0.333333    0.333333\n",
       "2  1233961        0.333333        0.333333    0.333333"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub = pd.read_csv(\"/kaggle/input/lmsys-chatbot-arena/sample_submission.csv\")\n",
    "\n",
    "if len(test)!=VALIDATE:\n",
    "    sub[[\"winner_model_a\",\"winner_model_b\",\"winner_tie\"]] = results\n",
    "    \n",
    "sub.to_csv(\"submission.csv\",index=False)\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f08fa83",
   "metadata": {
    "papermill": {
     "duration": 0.016989,
     "end_time": "2024-07-18T10:20:56.924478",
     "exception": false,
     "start_time": "2024-07-18T10:20:56.907489",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Compute CV Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8a74fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:20:56.959874Z",
     "iopub.status.busy": "2024-07-18T10:20:56.959631Z",
     "iopub.status.idle": "2024-07-18T10:20:56.971535Z",
     "shell.execute_reply": "2024-07-18T10:20:56.970685Z"
    },
    "papermill": {
     "duration": 0.032306,
     "end_time": "2024-07-18T10:20:56.973963",
     "exception": false,
     "start_time": "2024-07-18T10:20:56.941657",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 3)\n"
     ]
    }
   ],
   "source": [
    "if len(test)==VALIDATE:\n",
    "    true = test[['winner_model_a','winner_model_b','winner_tie']].values\n",
    "    print(true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "93f9cc85",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-18T10:20:57.010991Z",
     "iopub.status.busy": "2024-07-18T10:20:57.010732Z",
     "iopub.status.idle": "2024-07-18T10:20:57.677978Z",
     "shell.execute_reply": "2024-07-18T10:20:57.676945Z"
    },
    "papermill": {
     "duration": 0.687885,
     "end_time": "2024-07-18T10:20:57.680116",
     "exception": false,
     "start_time": "2024-07-18T10:20:56.992231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV loglosss is 0.9413079497822053\n"
     ]
    }
   ],
   "source": [
    "if len(test)==VALIDATE:\n",
    "    from sklearn.metrics import log_loss\n",
    "    print(f\"CV loglosss is {log_loss(true,results)}\" )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 4871830,
     "sourceId": 8218776,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4746046,
     "sourceId": 8300737,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5409557,
     "sourceId": 8982890,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30747,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 466.834885,
   "end_time": "2024-07-18T10:21:04.421001",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-07-18T10:13:17.586116",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
