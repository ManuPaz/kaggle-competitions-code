{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d4d6439",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T13:39:35.823073Z",
     "iopub.status.busy": "2023-10-10T13:39:35.822366Z",
     "iopub.status.idle": "2023-10-10T13:40:10.582235Z",
     "shell.execute_reply": "2023-10-10T13:40:10.581105Z"
    },
    "papermill": {
     "duration": 34.766313,
     "end_time": "2023-10-10T13:40:10.584525",
     "exception": false,
     "start_time": "2023-10-10T13:39:35.818212",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ./datasets-2.14.4-py3-none-any.whl\r\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (1.23.5)\r\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (11.0.0)\r\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.3.6)\r\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (1.5.3)\r\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2.31.0)\r\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (4.65.0)\r\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.2.0)\r\n",
      "Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.70.14)\r\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (2023.6.0)\r\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (3.8.4)\r\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (0.16.4)\r\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (21.3)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets==2.14.4) (6.0)\r\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (23.1.0)\r\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (3.1.0)\r\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (6.0.4)\r\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (4.0.2)\r\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.9.2)\r\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.3)\r\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.14.4) (1.3.1)\r\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (3.12.2)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.4) (4.6.3)\r\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets==2.14.4) (3.0.9)\r\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (1.26.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets==2.14.4) (2023.5.7)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2.8.2)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.14.4) (2023.3)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.4) (1.16.0)\r\n",
      "Installing collected packages: datasets\r\n",
      "  Attempting uninstall: datasets\r\n",
      "    Found existing installation: datasets 2.1.0\r\n",
      "    Uninstalling datasets-2.1.0:\r\n",
      "      Successfully uninstalled datasets-2.1.0\r\n",
      "Successfully installed datasets-2.14.4\r\n"
     ]
    }
   ],
   "source": [
    "!cp /kaggle/input/datasets-wheel/datasets-2.14.4-py3-none-any.whl /kaggle/working\n",
    "!pip install  /kaggle/working/datasets-2.14.4-py3-none-any.whl\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "274c1dbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T13:40:10.592117Z",
     "iopub.status.busy": "2023-10-10T13:40:10.591821Z",
     "iopub.status.idle": "2023-10-10T13:40:45.642393Z",
     "shell.execute_reply": "2023-10-10T13:40:45.640739Z"
    },
    "papermill": {
     "duration": 35.066655,
     "end_time": "2023-10-10T13:40:45.654301",
     "exception": false,
     "start_time": "2023-10-10T13:40:10.587646",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp -r /kaggle/input/stem-wiki-cohere-no-emb /kaggle/working\n",
    "!cp -r /kaggle/input/all-paraphs-parsed-expanded /kaggle/working/\n",
    "!cp -r /kaggle/input/new-contexts-3/context_paragraphs_204154.hf /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe640691",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T13:40:45.723254Z",
     "iopub.status.busy": "2023-10-10T13:40:45.722412Z",
     "iopub.status.idle": "2023-10-10T13:41:00.889581Z",
     "shell.execute_reply": "2023-10-10T13:41:00.888677Z"
    },
    "papermill": {
     "duration": 15.228608,
     "end_time": "2023-10-10T13:41:00.891636",
     "exception": false,
     "start_time": "2023-10-10T13:40:45.663028",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import ctypes\n",
    "libc = ctypes.CDLL(\"libc.so.6\")\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Union\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datasets import concatenate_datasets\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import gc\n",
    "from transformers import LongformerTokenizer, LongformerForMultipleChoice\n",
    "import transformers\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import unicodedata\n",
    "import gc\n",
    "import pickle\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc687b14",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T13:41:00.898886Z",
     "iopub.status.busy": "2023-10-10T13:41:00.898617Z",
     "iopub.status.idle": "2023-10-10T13:41:00.919621Z",
     "shell.execute_reply": "2023-10-10T13:41:00.918737Z"
    },
    "papermill": {
     "duration": 0.026668,
     "end_time": "2023-10-10T13:41:00.921305",
     "exception": false,
     "start_time": "2023-10-10T13:41:00.894637",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def SplitList(mylist, chunk_size):\n",
    "    return [mylist[offs:offs+chunk_size] for offs in range(0, len(mylist), chunk_size)]\n",
    "\n",
    "def get_relevant_documents_parsed(df_valid):\n",
    "    df_chunk_size=600\n",
    "    paraphs_parsed_dataset = load_from_disk(\"/kaggle/working/all-paraphs-parsed-expanded\")\n",
    "    my_context = load_from_disk(\"/kaggle/working/context_paragraphs_204154.hf\")\n",
    "    print(len(paraphs_parsed_dataset),len( my_context ))\n",
    "    paraphs_parsed_dataset =concatenate_datasets([paraphs_parsed_dataset , my_context])\n",
    "    print(len(  paraphs_parsed_dataset))\n",
    "    modified_texts = paraphs_parsed_dataset.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              f\"{example['title']} {example['section']} {example['text']}\".replace('\\n',\" \")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"title\"],\n",
    "                         paraphs_parsed_dataset[idx.item()][\"text\"],\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def get_relevant_documents(df_valid):\n",
    "    df_chunk_size=800\n",
    "    \n",
    "    cohere_dataset_filtered = load_from_disk(\"/kaggle/working/stem-wiki-cohere-no-emb\")\n",
    "    modified_texts = cohere_dataset_filtered.map(lambda example:\n",
    "                                             {'temp_text':\n",
    "                                              unicodedata.normalize(\"NFKD\", f\"{example['title']} {example['text']}\").replace('\"',\"\")},\n",
    "                                             num_proc=2)[\"temp_text\"]\n",
    "    \n",
    "    all_articles_indices = []\n",
    "    all_articles_values = []\n",
    "    for idx in tqdm(range(0, df_valid.shape[0], df_chunk_size)):\n",
    "        df_valid_ = df_valid.iloc[idx: idx+df_chunk_size]\n",
    "    \n",
    "        articles_indices, merged_top_scores = retrieval(df_valid_, modified_texts)\n",
    "        all_articles_indices.append(articles_indices)\n",
    "        all_articles_values.append(merged_top_scores)\n",
    "        \n",
    "    article_indices_array =  np.concatenate(all_articles_indices, axis=0)\n",
    "    articles_values_array = np.concatenate(all_articles_values, axis=0).reshape(-1)\n",
    "    \n",
    "    top_per_query = article_indices_array.shape[1]\n",
    "    articles_flatten = [(\n",
    "                         articles_values_array[index],\n",
    "                         cohere_dataset_filtered[idx.item()][\"title\"],\n",
    "                         unicodedata.normalize(\"NFKD\", cohere_dataset_filtered[idx.item()][\"text\"]),\n",
    "                        )\n",
    "                        for index,idx in enumerate(article_indices_array.reshape(-1))]\n",
    "    retrieved_articles = SplitList(articles_flatten, top_per_query)\n",
    "    return retrieved_articles\n",
    "\n",
    "\n",
    "\n",
    "def retrieval(df_valid, modified_texts):\n",
    "    \n",
    "    corpus_df_valid = df_valid.apply(lambda row:\n",
    "                                     f'{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"prompt\"]}\\n{row[\"A\"]}\\n{row[\"B\"]}\\n{row[\"C\"]}\\n{row[\"D\"]}\\n{row[\"E\"]}',\n",
    "                                     axis=1).values\n",
    "    vectorizer1 = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b\",\n",
    "                                 stop_words=stop_words,sublinear_tf=True)\n",
    "    vectorizer1.fit(corpus_df_valid)\n",
    "    vocab_df_valid = vectorizer1.get_feature_names_out()\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,2),\n",
    "                                 token_pattern=r\"(?u)\\b[\\w/.-]+\\b\",\n",
    "                                 stop_words=stop_words,\n",
    "                                 vocabulary=vocab_df_valid,sublinear_tf=True)\n",
    "    vectorizer.fit(modified_texts[:6_000_000])\n",
    "    corpus_tf_idf = vectorizer.transform(corpus_df_valid)\n",
    "    \n",
    "    print(f\"length of vectorizer vocab is {len(vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "    chunk_size = 100000\n",
    "    top_per_chunk = 10\n",
    "    top_per_query = 8\n",
    "\n",
    "    all_chunk_top_indices = []\n",
    "    all_chunk_top_values = []\n",
    "\n",
    "    for idx in tqdm(range(0, len(modified_texts), chunk_size)):\n",
    "        wiki_vectors = vectorizer.transform(modified_texts[idx: idx+chunk_size])\n",
    "        temp_scores = (corpus_tf_idf * wiki_vectors.T).toarray()\n",
    "        chunk_top_indices = temp_scores.argpartition(-top_per_chunk, axis=1)[:, -top_per_chunk:]\n",
    "        chunk_top_values = temp_scores[np.arange(temp_scores.shape[0])[:, np.newaxis], chunk_top_indices]\n",
    "\n",
    "        all_chunk_top_indices.append(chunk_top_indices + idx)\n",
    "        all_chunk_top_values.append(chunk_top_values)\n",
    "\n",
    "    top_indices_array = np.concatenate(all_chunk_top_indices, axis=1)\n",
    "    top_values_array = np.concatenate(all_chunk_top_values, axis=1)\n",
    "    \n",
    "    merged_top_scores = np.sort(top_values_array, axis=1)[:,-top_per_query:]\n",
    "    merged_top_indices = top_values_array.argsort(axis=1)[:,-top_per_query:]\n",
    "    articles_indices = top_indices_array[np.arange(top_indices_array.shape[0])[:, np.newaxis], merged_top_indices]\n",
    "    \n",
    "    return articles_indices, merged_top_scores\n",
    "\n",
    "\n",
    "def prepare_answering_input(\n",
    "        tokenizer, \n",
    "        question,  \n",
    "        options,   \n",
    "        context,   \n",
    "        max_seq_length=4096,\n",
    "    ):\n",
    "    c_plus_q   = context + ' ' + tokenizer.bos_token + ' ' + question\n",
    "    c_plus_q_4 = [c_plus_q] * len(options)\n",
    "    tokenized_examples = tokenizer(\n",
    "        c_plus_q_4, options,\n",
    "        max_length=max_seq_length,\n",
    "        padding=\"longest\",\n",
    "        truncation=False,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    input_ids = tokenized_examples['input_ids'].unsqueeze(0)\n",
    "    attention_mask = tokenized_examples['attention_mask'].unsqueeze(0)\n",
    "    example_encoded = {\n",
    "        \"input_ids\": input_ids.to(model.device.index),\n",
    "        \"attention_mask\": attention_mask.to(model.device.index),\n",
    "    }\n",
    "    return example_encoded\n",
    "stop_words = ['each', 'you', 'the', 'use', 'used',\n",
    "                  'where', 'themselves', 'nor', \"it's\", 'how', \"don't\", 'just', 'your',\n",
    "                  'about', 'himself', 'with', \"weren't\", 'hers', \"wouldn't\", 'more', 'its', 'were',\n",
    "                  'his', 'their', 'then', 'been', 'myself', 're', 'not',\n",
    "                  'ours', 'will', 'needn', 'which', 'here', 'hadn', 'it', 'our', 'there', 'than',\n",
    "                  'most', \"couldn't\", 'both', 'some', 'for', 'up', 'couldn', \"that'll\",\n",
    "                  \"she's\", 'over', 'this', 'now', 'until', 'these', 'few', 'haven',\n",
    "                  'of', 'wouldn', 'into', 'too', 'to', 'very', 'shan', 'before', 'the', 'they',\n",
    "                  'between', \"doesn't\", 'are', 'was', 'out', 'we', 'me',\n",
    "                  'after', 'has', \"isn't\", 'have', 'such', 'should', 'yourselves', 'or', 'during', 'herself',\n",
    "                  'doing', 'in', \"shouldn't\", \"won't\", 'when', 'do', 'through', 'she',\n",
    "                  'having', 'him', \"haven't\", 'against', 'itself', 'that',\n",
    "                  'did', 'theirs', 'can', 'those',\n",
    "                  'own', 'so', 'and', 'who', \"you've\", 'yourself', 'her', 'he', 'only',\n",
    "                  'what', 'ourselves', 'again', 'had', \"you'd\", 'is', 'other',\n",
    "                  'why', 'while', 'from', 'them', 'if', 'above', 'does', 'whom',\n",
    "                  'yours', 'but', 'being', \"wasn't\", 'be']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "219c8241",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T13:41:00.928048Z",
     "iopub.status.busy": "2023-10-10T13:41:00.927558Z",
     "iopub.status.idle": "2023-10-10T14:01:45.947178Z",
     "shell.execute_reply": "2023-10-10T14:01:45.945734Z"
    },
    "papermill": {
     "duration": 1245.029425,
     "end_time": "2023-10-10T14:01:45.953420",
     "exception": false,
     "start_time": "2023-10-10T13:41:00.923995",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2101279 3294990\n",
      "5396269\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9935fcd6a15942b08aa646a162a6ff92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/5396269 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of vectorizer vocab is 11077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0/54 [00:00<?, ?it/s]\u001b[A\n",
      "  2%|▏         | 1/54 [00:12<10:38, 12.04s/it]\u001b[A\n",
      "  4%|▎         | 2/54 [00:24<10:50, 12.50s/it]\u001b[A\n",
      "  6%|▌         | 3/54 [00:36<10:28, 12.33s/it]\u001b[A\n",
      "  7%|▋         | 4/54 [00:49<10:10, 12.20s/it]\u001b[A\n",
      "  9%|▉         | 5/54 [01:01<10:08, 12.43s/it]\u001b[A\n",
      " 11%|█         | 6/54 [01:13<09:52, 12.34s/it]\u001b[A\n",
      " 13%|█▎        | 7/54 [01:26<09:36, 12.26s/it]\u001b[A\n",
      " 15%|█▍        | 8/54 [01:38<09:32, 12.44s/it]\u001b[A\n",
      " 17%|█▋        | 9/54 [01:51<09:15, 12.35s/it]\u001b[A\n",
      " 19%|█▊        | 10/54 [02:04<09:11, 12.53s/it]\u001b[A\n",
      " 20%|██        | 11/54 [02:15<08:50, 12.34s/it]\u001b[A\n",
      " 22%|██▏       | 12/54 [02:27<08:34, 12.26s/it]\u001b[A\n",
      " 24%|██▍       | 13/54 [02:40<08:29, 12.42s/it]\u001b[A\n",
      " 26%|██▌       | 14/54 [02:53<08:15, 12.38s/it]\u001b[A\n",
      " 28%|██▊       | 15/54 [03:05<08:09, 12.55s/it]\u001b[A\n",
      " 30%|██▉       | 16/54 [03:18<07:51, 12.40s/it]\u001b[A\n",
      " 31%|███▏      | 17/54 [03:30<07:35, 12.31s/it]\u001b[A\n",
      " 33%|███▎      | 18/54 [03:43<07:29, 12.48s/it]\u001b[A\n",
      " 35%|███▌      | 19/54 [03:55<07:11, 12.34s/it]\u001b[A\n",
      " 37%|███▋      | 20/54 [04:07<06:59, 12.33s/it]\u001b[A\n",
      " 39%|███▉      | 21/54 [04:19<06:47, 12.34s/it]\u001b[A\n",
      " 41%|████      | 22/54 [04:24<05:26, 10.21s/it]\u001b[A\n",
      " 43%|████▎     | 23/54 [04:30<04:29,  8.70s/it]\u001b[A\n",
      " 44%|████▍     | 24/54 [04:35<03:49,  7.63s/it]\u001b[A\n",
      " 46%|████▋     | 25/54 [04:43<03:45,  7.77s/it]\u001b[A\n",
      " 48%|████▊     | 26/54 [04:56<04:21,  9.33s/it]\u001b[A\n",
      " 50%|█████     | 27/54 [05:08<04:31, 10.05s/it]\u001b[A\n",
      " 52%|█████▏    | 28/54 [05:15<03:59,  9.20s/it]\u001b[A\n",
      " 54%|█████▎    | 29/54 [05:21<03:30,  8.41s/it]\u001b[A\n",
      " 56%|█████▌    | 30/54 [05:28<03:08,  7.87s/it]\u001b[A\n",
      " 57%|█████▋    | 31/54 [05:34<02:49,  7.35s/it]\u001b[A\n",
      " 59%|█████▉    | 32/54 [05:40<02:34,  7.01s/it]\u001b[A\n",
      " 61%|██████    | 33/54 [05:48<02:29,  7.10s/it]\u001b[A\n",
      " 63%|██████▎   | 34/54 [05:54<02:18,  6.92s/it]\u001b[A\n",
      " 65%|██████▍   | 35/54 [06:01<02:09,  6.79s/it]\u001b[A\n",
      " 67%|██████▋   | 36/54 [06:07<02:00,  6.72s/it]\u001b[A\n",
      " 69%|██████▊   | 37/54 [06:14<01:52,  6.61s/it]\u001b[A\n",
      " 70%|███████   | 38/54 [06:21<01:49,  6.83s/it]\u001b[A\n",
      " 72%|███████▏  | 39/54 [06:27<01:40,  6.70s/it]\u001b[A\n",
      " 74%|███████▍  | 40/54 [06:34<01:33,  6.65s/it]\u001b[A\n",
      " 76%|███████▌  | 41/54 [06:40<01:25,  6.58s/it]\u001b[A\n",
      " 78%|███████▊  | 42/54 [06:47<01:20,  6.68s/it]\u001b[A\n",
      " 80%|███████▉  | 43/54 [06:54<01:12,  6.59s/it]\u001b[A\n",
      " 81%|████████▏ | 44/54 [07:00<01:05,  6.58s/it]\u001b[A\n",
      " 83%|████████▎ | 45/54 [07:06<00:58,  6.52s/it]\u001b[A\n",
      " 85%|████████▌ | 46/54 [07:13<00:52,  6.52s/it]\u001b[A\n",
      " 87%|████████▋ | 47/54 [07:20<00:46,  6.71s/it]\u001b[A\n",
      " 89%|████████▉ | 48/54 [07:27<00:40,  6.67s/it]\u001b[A\n",
      " 91%|█████████ | 49/54 [07:33<00:32,  6.56s/it]\u001b[A\n",
      " 93%|█████████▎| 50/54 [07:40<00:26,  6.58s/it]\u001b[A\n",
      " 94%|█████████▍| 51/54 [07:46<00:19,  6.58s/it]\u001b[A\n",
      " 96%|█████████▋| 52/54 [07:54<00:13,  6.79s/it]\u001b[A\n",
      " 98%|█████████▊| 53/54 [08:00<00:06,  6.63s/it]\u001b[A\n",
      "100%|██████████| 54/54 [08:06<00:00,  9.01s/it]\n",
      "100%|██████████| 1/1 [15:39<00:00, 939.28s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_valid=pd.read_csv(\"/kaggle/input/kaggle-llm-science-exam/test.csv\")\n",
    "retrieved_articles_parsed = get_relevant_documents_parsed(df_valid)\n",
    "gc.collect()\n",
    "data_=retrieved_articles_parsed \n",
    "contexts=np.array([ \"\\n\".join([data_[i][e][2] for e in range(len(data_[0])    )])  for i in range(len(data_)) ])\n",
    "df_valid[\"context\"]=contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d330342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T14:01:46.004416Z",
     "iopub.status.busy": "2023-10-10T14:01:46.004099Z",
     "iopub.status.idle": "2023-10-10T14:16:48.612235Z",
     "shell.execute_reply": "2023-10-10T14:16:48.611333Z"
    },
    "papermill": {
     "duration": 902.647816,
     "end_time": "2023-10-10T14:16:48.625726",
     "exception": false,
     "start_time": "2023-10-10T14:01:45.977910",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6ffc58220f7466087cca8cb5ce9d135",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DebertaV2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9925\n"
     ]
    }
   ],
   "source": [
    "MAX_INPUT = 2500\n",
    "def competition_score(y_true, y_pred):\n",
    "\n",
    "        \"\"\"\n",
    "        Obtaining score for the model\n",
    "        \"\"\"\n",
    "\n",
    "        ap_at_3 = 0.0\n",
    "        for i in range(len(y_true)):\n",
    "            if y_true[i]==  y_pred[i][0]:\n",
    "                ap_at_3 += 1\n",
    "            elif y_true[i]==  y_pred[i][1]:\n",
    "                ap_at_3 += 1/2\n",
    "            elif y_true[i]==  y_pred[i][2]:\n",
    "                ap_at_3 += 1/3\n",
    "\n",
    "\n",
    "        map3 = ap_at_3 / len(y_true)\n",
    "        return map3\n",
    "\n",
    "def predictions_to_map_output(predictions):\n",
    "        sorted_answer_indices = np.argsort(-predictions) # Sortting indices in descending order\n",
    "        top_answer_indices = sorted_answer_indices[:,:3] # Taking the first three indices for each row\n",
    "        top_answers = np.vectorize(index_to_option.get)(top_answer_indices) # Transforming indices to options - i.e., 0 --> A\n",
    "        return  top_answers\n",
    "def compute_metrics(eval_preds):\n",
    "        logits, labels = eval_preds\n",
    "        y_pred = predictions_to_map_output(logits)\n",
    "        y_true = [index_to_option[label] for label in labels]\n",
    "        print( y_true)\n",
    "        return {'Map@3': np.round(competition_score(y_true, y_pred), 3)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_wrapper(tokenizer,MAX_INPUT=MAX_INPUT):\n",
    "    def preprocess(example):\n",
    "\n",
    "        first_sentence = [example['prompt']] * 5 # Repeating the same question 5 times\n",
    "        second_sentence = [] # Creating list of possible answers\n",
    "        for option in options:\n",
    "            second_sentence.append(example[option])\n",
    "\n",
    "        # The tokenizer converts the question and answers into 'tokens'.\n",
    "        # 'tokens' are simply a sequence of integers in which each specific integer corresponds to\n",
    "        # a word or subword that BERT is capable of comprehending\n",
    "        tokenized_example = tokenizer(first_sentence, second_sentence, truncation=True)\n",
    "\n",
    "        # Indexing label - A,B,C,D or E - to either 0, 1, 2, 3, 4, or 5\n",
    "        tokenized_example['label'] = option_to_index[example['answer']]\n",
    "\n",
    "        # tokenized_example returns:\n",
    "            # input_ids --> List of lists represents the tokens\n",
    "            # token_type_ids --> A list of lists indicating whether each token belongs to the 1st or 2nd sentence\n",
    "            # attention_mask --> List of list where each inner list is either 1 or 0. 1 are not padding tokens, 0 are padding tokens\n",
    "            # label --> The index for the correct option\n",
    "        return tokenized_example\n",
    "\n",
    "    def preprocess_with_context(example):\n",
    "\n",
    "        first_sentence = [ \"[CLS] \" + example['context'] ] * 5\n",
    "        second_sentences = [\" #### \" + example['prompt'] + \" [SEP] \" + example[option] + \" [SEP]\" for option in 'ABCDE']\n",
    "\n",
    "        #tokenized_example = tokenizer(first_sentence, second_sentences, truncation=True)\n",
    "        tokenized_example = tokenizer(first_sentence, second_sentences, truncation='only_first',\n",
    "                                        max_length=MAX_INPUT, add_special_tokens=False)\n",
    "        tokenized_example['label'] = option_to_index[example['answer']]\n",
    "\n",
    "        return tokenized_example\n",
    "\n",
    "    return preprocess_with_context\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    '''\n",
    "    This class is designed to handle the formatting and batching the data for mutiple-choice tasks\n",
    "    '''\n",
    "\n",
    "    # The tokenizer to be used for tokenizing the data\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "\n",
    "    # The strategy to be used for padding the data\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "\n",
    "    # The maximum length for any input sequence\n",
    "    max_length: Optional[int] = None\n",
    "\n",
    "    # If provided, pad the sequences to a multiple of this value\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        # Finding the correct label key in the features\n",
    "        label_name = \"label\" if 'label' in features[0].keys() else 'labels'\n",
    "\n",
    "        # Extracting the labels and removing them from features\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "\n",
    "        # Obtaining batch size\n",
    "        batch_size = len(features)\n",
    "\n",
    "        # Obtaining number of choices\n",
    "        num_choices = len(features[0]['input_ids'])\n",
    "\n",
    "        # Reestructuring features so each question-choice pair becomes a separate example\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        # Padding all sequences to the same length\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        # Reshaping the batch back into the original format\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "\n",
    "        # Adding the labels back into the batch as a tensor\n",
    "        batch['labels'] = torch.tensor(labels,\n",
    "                                    dtype=torch.int64)\n",
    "\n",
    "        # Returning the batch\n",
    "        return batch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "test_df = df_valid.copy()\n",
    "test_df.index = list(range(len(test_df)))\n",
    "test_df['id'] = list(range(len(test_df)))\n",
    "#test_df[\"prompt\"] = test_df[\"context\"].apply(lambda x: x[:2000]) + \" #### \" +  test_df[\"prompt\"]\n",
    "test_df['answer'] = 'A'\n",
    "test_ds=Dataset.from_pandas(test_df)\n",
    "\n",
    "\n",
    "\n",
    "r_cols= [e for e in ['prompt', 'A', 'B', 'C', 'D', 'E', 'answer', '__index_level_0__','context','source','id']  if e in test_ds.features]\n",
    "\n",
    "\n",
    "\n",
    "model_dir = \"/kaggle/input/llm-science-run-context-2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "options = 'ABCDE'\n",
    "indices = list(range(5))\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)}\n",
    "index_to_option = {index: option for option, index in zip(options, indices)}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokenized_test_dataset= test_ds.map(preprocess_wrapper(tokenizer,MAX_INPUT=MAX_INPUT), batched=False,\n",
    "                            remove_columns=r_cols)\n",
    "data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "\n",
    "def softmax(x):\n",
    "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return e_x / e_x.sum(axis=-1, keepdims = True)    \n",
    "\n",
    "\n",
    "test_predictions1 = []\n",
    "for batch in test_dataloader:\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    test_predictions1.append(outputs.logits.cpu().detach())\n",
    "\n",
    "test_predictions1 = torch.cat(test_predictions1)\n",
    "test_predictions1 = test_predictions1.numpy()\n",
    "\n",
    "\n",
    "options = 'ABCDE'\n",
    "indices = list(range(5)) # Indexing 0, 1, 2, 3, 4 for each option\n",
    "\n",
    "\n",
    "option_to_index = {option: index for option, index in zip(options, indices)} # Converting options to indices '''A to 0'''\n",
    "index_to_option = {index: option for option, index in zip(options, indices)} # Converting indices to options '''0 to A'''\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model_dir = \"/kaggle/input/big-models/checkpoint-23000\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "model.eval()\n",
    "\n",
    "test_dataloader = DataLoader(tokenized_test_dataset, batch_size=1, shuffle=False, collate_fn=data_collator)\n",
    "\n",
    "test_predictions2 = []\n",
    "for batch in test_dataloader:\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    test_predictions2.append(outputs.logits.cpu().detach())\n",
    "\n",
    "test_predictions2 = torch.cat(test_predictions2)\n",
    "test_predictions2 = test_predictions2.numpy()\n",
    "\n",
    "\n",
    "model_dir = \"/kaggle/input/change-validation/checkpoint18576/checkpoints/checkpoint-18576\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = AutoModelForMultipleChoice.from_pretrained(model_dir).cuda()\n",
    "model.eval()\n",
    "\n",
    "\n",
    "\n",
    "test_predictions3 = []\n",
    "for batch in test_dataloader:\n",
    "    for k in batch.keys():\n",
    "        batch[k] = batch[k].cuda()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    test_predictions3.append(outputs.logits.cpu().detach())\n",
    "\n",
    "test_predictions3 = torch.cat(test_predictions3)\n",
    "test_predictions3 = test_predictions3.numpy()\n",
    "\n",
    "\n",
    "test_predictions = 0.2*softmax (test_predictions1) + 0.6*softmax(test_predictions2) + 0.2*softmax(test_predictions3) \n",
    "\n",
    "predictions_as_ids = np.argsort(-test_predictions, 1)\n",
    "\n",
    "predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "\n",
    "test_df['prediction'] = [\n",
    "    ' '.join(row) for row in predictions_as_answer_letters[:, :3]\n",
    "]\n",
    "\n",
    "\n",
    "submission = test_df[['id', 'prediction']]\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/60k-data-with-context-v2/train_with_context2.csv') # Loading data\n",
    "test_df[\"answer\"] = df[\"answer\"]\n",
    "\n",
    "\n",
    "print(competition_score(test_df[\"answer\"], predictions_as_answer_letters))\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54e63b41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-10-10T14:16:48.647379Z",
     "iopub.status.busy": "2023-10-10T14:16:48.647115Z",
     "iopub.status.idle": "2023-10-10T14:16:48.701616Z",
     "shell.execute_reply": "2023-10-10T14:16:48.700439Z"
    },
    "papermill": {
     "duration": 0.067416,
     "end_time": "2023-10-10T14:16:48.703531",
     "exception": false,
     "start_time": "2023-10-10T14:16:48.636115",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99\n",
      "0.9875\n",
      "0.9825\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for pred in [test_predictions1,test_predictions2,test_predictions3]:\n",
    "    predictions_as_ids = np.argsort(-pred, 1)\n",
    "\n",
    "    predictions_as_answer_letters = np.array(list('ABCDE'))[predictions_as_ids]\n",
    "\n",
    "\n",
    "    df = pd.read_csv('/kaggle/input/60k-data-with-context-v2/train_with_context2.csv') # Loading data\n",
    "    test_df[\"answer\"] = df[\"answer\"]\n",
    "\n",
    "\n",
    "    print(competition_score(test_df[\"answer\"], predictions_as_answer_letters))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dac84cf",
   "metadata": {
    "papermill": {
     "duration": 0.010043,
     "end_time": "2023-10-10T14:16:48.724162",
     "exception": false,
     "start_time": "2023-10-10T14:16:48.714119",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Thanks to https://www.kaggle.com/code/mewmlelswm/interesting-combination-of-average-experiments/notebook\n",
    "\n",
    "          https://www.kaggle.com/code/cdeotte/how-to-train-open-book-model-part-2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2244.591256,
   "end_time": "2023-10-10T14:16:52.082549",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-10T13:39:27.491293",
   "version": "2.4.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "04f0cf8cc9bc4cbc9d543417625ca761": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "0e567917ded9474490ec963e19273d8c": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_efd25ea4b87b487198b5795eaee10f5e",
       "max": 200.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_96c546a491ea48f58c02333c38f1c507",
       "value": 200.0
      }
     },
     "0e9410d751b84c9eb6387f839382fab6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_e4e0cc2c831f4db4bfb36bd00e9c8af4",
       "max": 5396269.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_8738f2cd42394d6e9a518fa0b25d3175",
       "value": 5396269.0
      }
     },
     "103a43790f2e4cacb5f9d3343f476753": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "28a6a361b2254291b9d165f70940d522": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "521308f18ab64658b52a0b46e38be04f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "587c174c45b6485393853f3e0b2ee3b4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_7067e11605774d5c8ae42ef866172934",
       "placeholder": "​",
       "style": "IPY_MODEL_103a43790f2e4cacb5f9d3343f476753",
       "value": " 5396269/5396269 [04:38&lt;00:00, 19357.31 examples/s]"
      }
     },
     "6a543598334a476189ef87814e3887ce": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "7067e11605774d5c8ae42ef866172934": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "777e8b6ef21a41aabb0d6265b4269e32": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "8738f2cd42394d6e9a518fa0b25d3175": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "88686c57a122456791c9f2f1b0d81a6e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_04f0cf8cc9bc4cbc9d543417625ca761",
       "placeholder": "​",
       "style": "IPY_MODEL_e84dd862fa5a4595b595c69b5ff3281b",
       "value": "Map: 100%"
      }
     },
     "8fdda597be864e068d628ba143188ba3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_ba698e73d0e14e3ebc55e17444a9124a",
       "placeholder": "​",
       "style": "IPY_MODEL_777e8b6ef21a41aabb0d6265b4269e32",
       "value": "Map (num_proc=2): 100%"
      }
     },
     "96c546a491ea48f58c02333c38f1c507": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "9935fcd6a15942b08aa646a162a6ff92": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_8fdda597be864e068d628ba143188ba3",
        "IPY_MODEL_0e9410d751b84c9eb6387f839382fab6",
        "IPY_MODEL_587c174c45b6485393853f3e0b2ee3b4"
       ],
       "layout": "IPY_MODEL_6a543598334a476189ef87814e3887ce"
      }
     },
     "b6ffc58220f7466087cca8cb5ce9d135": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_88686c57a122456791c9f2f1b0d81a6e",
        "IPY_MODEL_0e567917ded9474490ec963e19273d8c",
        "IPY_MODEL_f5f561ba0c6a429dbdc0075ad6532736"
       ],
       "layout": "IPY_MODEL_fe5a13257e9b4bc3bd82f79adb9883e8"
      }
     },
     "ba698e73d0e14e3ebc55e17444a9124a": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e4e0cc2c831f4db4bfb36bd00e9c8af4": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "e84dd862fa5a4595b595c69b5ff3281b": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "efd25ea4b87b487198b5795eaee10f5e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "f5f561ba0c6a429dbdc0075ad6532736": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.5.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.5.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.5.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_28a6a361b2254291b9d165f70940d522",
       "placeholder": "​",
       "style": "IPY_MODEL_521308f18ab64658b52a0b46e38be04f",
       "value": " 200/200 [00:04&lt;00:00, 46.65 examples/s]"
      }
     },
     "fe5a13257e9b4bc3bd82f79adb9883e8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.2.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.2.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.2.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
