{"cells":[{"cell_type":"markdown","metadata":{},"source":["# CAFA 5 Competition : Protein Function Prediction"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from utils_local import *"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["model_name=\"cn\" \n","TRAIN = True\n","embeddings_source1=\"T5\"\n","embeddings_source2=\"EMS2\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","\n","PATHS = {\n","    \"PATH\": PATH, \n","    \"PATH_LABELS\": PATH_LABELS,\n","    \"PATH_DATAFRAMES_2D\": PATH_DATAFRAMES_2D,\n","    \"CHECKPOINT\": {\"local\":'../results/models_folds/models_{}', \"kaggle\": '/kaggle/working/models_{}'},\n","    \"PREDICTIONS\": {\"local\":'../results/preds_cafa_embeddings_bp_2emb/preds_fold{}.tsv', \"kaggle\": 'preds_fold{}.tsv'},\n","    \"LABELS\": {\"local\":'../results/preds_cafa_embeddings_bp_2emb/labels_used_fold{}.tsv', \"kaggle\": 'labels_used_fold{}.tsv'},\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:34.596617Z","iopub.status.busy":"2023-07-09T20:44:34.594974Z","iopub.status.idle":"2023-07-09T20:44:34.863600Z","shell.execute_reply":"2023-07-09T20:44:34.861636Z","shell.execute_reply.started":"2023-07-09T20:44:34.596563Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","sub = pd.read_csv(f\"{PATH}cafa-5-protein-function-prediction/sample_submission.tsv\", sep= \"\\t\", header = None)\n","sub.columns = [\"The Protein ID\", \"The Gene Ontology term (GO) ID\", \"Predicted link probability that GO appear in Protein\"]\n","sub.head(5)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:34.866412Z","iopub.status.busy":"2023-07-09T20:44:34.865929Z","iopub.status.idle":"2023-07-09T20:44:43.351553Z","shell.execute_reply":"2023-07-09T20:44:43.349961Z","shell.execute_reply.started":"2023-07-09T20:44:34.866377Z"},"trusted":true},"outputs":[],"source":["from Bio import SeqIO\n","print(\"Loading train set ProtBERT Embeddings...\")\n","fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n","print(\"Total Nb of Elements : \", len(list(fasta_train)))\n","fasta_test= SeqIO.parse(config.test_sequences_path, \"fasta\")\n","print(\"Total Nb of Elements : \", len(list(fasta_test)))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:43.355824Z","iopub.status.busy":"2023-07-09T20:44:43.355389Z","iopub.status.idle":"2023-07-09T20:44:43.414180Z","shell.execute_reply":"2023-07-09T20:44:43.412682Z","shell.execute_reply.started":"2023-07-09T20:44:43.355789Z"},"trusted":true},"outputs":[],"source":["weights=pd.read_csv(f\"{PATH}cafa-5-protein-function-prediction/IA.txt\",sep=\"\\t\",header=None,index_col=0).rename(columns={1:\"weight\"})\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:43.417238Z","iopub.status.busy":"2023-07-09T20:44:43.416666Z","iopub.status.idle":"2023-07-09T20:44:46.743934Z","shell.execute_reply":"2023-07-09T20:44:46.742643Z","shell.execute_reply.started":"2023-07-09T20:44:43.417187Z"},"trusted":true},"outputs":[],"source":[" labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n"," labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:46.750706Z","iopub.status.busy":"2023-07-09T20:44:46.749767Z","iopub.status.idle":"2023-07-09T20:44:47.640015Z","shell.execute_reply":"2023-07-09T20:44:47.638640Z","shell.execute_reply.started":"2023-07-09T20:44:46.750665Z"},"trusted":true},"outputs":[],"source":[" labels= labels.loc[~labels.term.isin(list(weights[weights.weight==0].index))]\n"," labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:47.643404Z","iopub.status.busy":"2023-07-09T20:44:47.642106Z","iopub.status.idle":"2023-07-09T20:44:47.649522Z","shell.execute_reply":"2023-07-09T20:44:47.648357Z","shell.execute_reply.started":"2023-07-09T20:44:47.643269Z"},"trusted":true},"outputs":[],"source":["# Directories for the different embedding vectors : \n","embeds_map = {\n","    \"T5\" : \"my-t5embeds\",\n","    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n","    \"EMS2\" : \"cafa-5-ems-2-embeddings-numpy\"\n","}\n","\n","# Length of the different embedding vectors :\n","embeds_dim = {\n","    \"T5\" : 1024,\n","    \"ProtBERT\" : 1024,\n","    \"EMS2\" : 1280\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:47.651450Z","iopub.status.busy":"2023-07-09T20:44:47.650774Z","iopub.status.idle":"2023-07-09T20:44:47.666170Z","shell.execute_reply":"2023-07-09T20:44:47.664625Z","shell.execute_reply.started":"2023-07-09T20:44:47.651400Z"},"trusted":true},"outputs":[],"source":[" LEARNING_RATE=0.001\n"," WARMUP_STEPS=7\n"," class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, initial_lr, warmup_steps=1):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.initial_lr = tf.cast(initial_lr, tf.float32)\n","        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32)\n","        return tf.math.minimum(self.initial_lr, self.initial_lr * (step/self.warmup_steps))  \n","    \n","    def get_config(self):\n","        return {\"initial_learning_rate\": self.initial_lr}\n","    \n","\n","'''\n","PredictionFnCallback is used for:\n","1. Loading validation data\n","2. FOGModel data preparation\n","3. Prediction\n","4. Scoring and save\n","\n","'''"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:47.669265Z","iopub.status.busy":"2023-07-09T20:44:47.668759Z","iopub.status.idle":"2023-07-09T20:44:47.683406Z","shell.execute_reply":"2023-07-09T20:44:47.682100Z","shell.execute_reply.started":"2023-07-09T20:44:47.669221Z"},"trusted":true},"outputs":[],"source":["def get_dataset( datatype, embeddings_source):\n","        ids_ = np.load(f\"{PATH}\" + embeds_map[\"T5\"] + \"/\" + datatype + \"_ids.npy\")\n","       \n","        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n","            embeds = np.load(f\"{PATH}\" + embeds_map[embeddings_source] + \"/\" + datatype + \"_embeddings.npy\")\n","            ids = np.load(f\"{PATH}\" + embeds_map[embeddings_source] + \"/\" + datatype + \"_ids.npy\")\n","            dic_ids={e:i for i, e in enumerate(ids)}\n","            indices=[dic_ids[e] for e in ids_]\n","            embeds=[ embeds[i] for i in indices]\n","        if embeddings_source == \"T5\":\n","            embeds = np.load(f\"{PATH}\"  + embeds_map[embeddings_source] + \"/\" + datatype + \"_embeds.npy\")\n","            ids = np.load(f\"{PATH}\"  + embeds_map[embeddings_source] + \"/\" + datatype + \"_ids.npy\")\n","            dic_ids={e:i for i, e in enumerate(ids)}\n","            indices=[dic_ids[e] for e in ids_]\n","            embeds=[ embeds[i] for i in indices]\n","\n","        if datatype==\"train\":\n","            df_labels=pd.read_pickle(PATH_LABELS)\n","            \n","            return embeds,ids,df_labels.loc[ids]\n","        return embeds,ids\n","        \n"," "]},{"cell_type":"markdown","metadata":{},"source":["# 6. Pytorch Models Architectures"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:47.686254Z","iopub.status.busy":"2023-07-09T20:44:47.685699Z","iopub.status.idle":"2023-07-09T20:44:47.711587Z","shell.execute_reply":"2023-07-09T20:44:47.710077Z","shell.execute_reply.started":"2023-07-09T20:44:47.686210Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","class MultiLayerPerceptron_v0(tf.keras.Model):\n","\n","    def __init__(self, num_classes):\n","        super(MultiLayerPerceptron, self).__init__()\n","                \n","         \n","        self.linear1 = tf.keras.layers.Dense(256)\n","        #self.activation1 = tf.keras.layers.ReLU()\n","        self.linear2 = tf.keras.layers.Dense(128)\n","        #self.activation2 = tf.keras.layers.ReLU()\n","        self.linear2 = tf.keras.layers.Dense(56)\n","        \n","        self.linear3 = tf.keras.layers.Dense(num_classes,activation=\"sigmoid\")\n","\n","    def call(self, x):\n","\n","        x = self.linear1(x)\n","        #x = self.activation1(x)\n","        x = self.linear2(x)\n","        # x = self.activation2(x)\n","        x = self.linear3(x)\n","        return x\n","import tensorflow as tf\n","\n","class ResidualBlock(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(ResidualBlock, self).__init__()\n","        self.fc1 = tf.keras.layers.Dense(units, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(units, activation='relu')\n","\n","    def call(self, x):\n","        residual = x\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = x + residual\n","        return x\n","\n","class MultiLayerPerceptron(tf.keras.Model):\n","    def __init__(self, num_classes):\n","        super(MultiLayerPerceptron, self).__init__()\n","        self.fc1 = tf.keras.layers.Dense(256, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n","        self.res_block1 = ResidualBlock(128)\n","        self.res_block2 = ResidualBlock(128)\n","        self.fc3 = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n","\n","    def call(self, x):\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.res_block1(x)\n","        x = self.res_block2(x)\n","        x = self.fc3(x)\n","        return x\n","\n","\n","\n","\n","#models_t5_1500_cnn_v0_2d\n","class CNN1D(tf.keras.Model):  #v0_2d\n","    def __init__(self, num_classes):\n","        super(CNN1D, self).__init__()\n","        # (batch_size, channels, embed_size)\n","        self.conv1 = tf.keras.layers.Conv1D(filters=3, kernel_size=3, dilation_rate=1, padding='same', activation='relu')\n","        # (batch_size, 3, embed_size)\n","        self.pool1 = tf.keras.layers.MaxPool1D(pool_size=2, strides=2)\n","        # (batch_size, 3, embed_size/2 = 512)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=3, kernel_size=3, dilation_rate=1, padding='same', activation='relu')\n","        # (batch_size, 8, embed_size/2 = 512)\n","        self.pool2 = tf.keras.layers.MaxPool1D(pool_size=2, strides=2)\n","        \n","        self.flatten=tf.keras.layers.Flatten()\n","        # (batch_size, 8, embed_size/4 = 256)\n","        self.fc1 = tf.keras.layers.Dense(units=128, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(units=num_classes,activation=\"sigmoid\")\n","\n","    def call(self, x):\n","        #x = tf.reshape(x, [-1, tf.shape(x)[1],1])\n","        x=self.conv1(x)\n","        x = self.pool1(x)\n","        x = self.pool2(self.conv2(x))\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Train the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:47.714389Z","iopub.status.busy":"2023-07-09T20:44:47.713791Z","iopub.status.idle":"2023-07-09T20:44:51.674690Z","shell.execute_reply":"2023-07-09T20:44:51.673072Z","shell.execute_reply.started":"2023-07-09T20:44:47.714256Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","\n","X = np.load(os.path.join(PATH_DATAFRAMES_2D,f\"embeds_{embeddings_source1}-{embeddings_source2}.npy\"))\n","y = pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_{   config. num_labels}.pkl\"))\n","#X=np.concatenate([X_train,X_validate])\n","#y=pd.concat([y_train,y_validate])\n","\n","num_classes=y.shape[1]\n","print(f\" Num classes {num_classes}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:51.677356Z","iopub.status.busy":"2023-07-09T20:44:51.676944Z","iopub.status.idle":"2023-07-09T20:44:51.719771Z","shell.execute_reply":"2023-07-09T20:44:51.718295Z","shell.execute_reply.started":"2023-07-09T20:44:51.677315Z"},"trusted":true},"outputs":[],"source":["display(y.head())"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:51.725166Z","iopub.status.busy":"2023-07-09T20:44:51.724752Z","iopub.status.idle":"2023-07-09T20:44:51.748505Z","shell.execute_reply":"2023-07-09T20:44:51.746938Z","shell.execute_reply.started":"2023-07-09T20:44:51.725136Z"},"trusted":true},"outputs":[],"source":["w_use=weights.loc[y.columns].values\n","w_use.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:51.751115Z","iopub.status.busy":"2023-07-09T20:44:51.750342Z","iopub.status.idle":"2023-07-09T20:44:59.519722Z","shell.execute_reply":"2023-07-09T20:44:59.517261Z","shell.execute_reply.started":"2023-07-09T20:44:51.751079Z"},"trusted":true},"outputs":[],"source":["labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n","terms=labels[[\"term\",\"aspect\"]].drop_duplicates().set_index(\"term\").loc[y.columns]\n","aspects={}\n","for aspect,df_terms in terms.groupby(\"aspect\"):\n","    terms_index=list(df_terms.index.values)\n","    display(df_terms.head(5))\n","    indices = list(np.where(np.isin(y.columns,    terms_index))[0])\n","    aspects[aspect]=indices\n","terms.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:59.524083Z","iopub.status.busy":"2023-07-09T20:44:59.523427Z","iopub.status.idle":"2023-07-09T20:44:59.535558Z","shell.execute_reply":"2023-07-09T20:44:59.534353Z","shell.execute_reply.started":"2023-07-09T20:44:59.524029Z"},"trusted":true},"outputs":[],"source":["for u in aspects.values():\n","    print(len(u))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:44:59.539081Z","iopub.status.busy":"2023-07-09T20:44:59.538530Z","iopub.status.idle":"2023-07-09T20:45:00.212119Z","shell.execute_reply":"2023-07-09T20:45:00.210412Z","shell.execute_reply.started":"2023-07-09T20:44:59.539041Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def compute_metrics_(tau_arr, g, pred, toi, n_gt, wn_gt=None, ic_arr=None):\n","\n","    metrics = np.zeros((len(tau_arr), 7), dtype='float')  # cov, pr, rc, wpr, wrc, ru, mi\n","\n","    for i, tau in enumerate(tau_arr):\n","\n","        p = solidify_prediction(pred[:, toi], tau)\n","\n","        # number of proteins with at least one term predicted with score >= tau\n","        metrics[i, 0] = (p.sum(axis=1) > 0).sum()\n","\n","        # Terms subsets\n","        intersection = np.logical_and(p, g)  # TP\n","\n","        \n","\n","        # Subsets size\n","        n_pred = p.sum(axis=1)\n","        n_intersection = intersection.sum(axis=1)\n","\n","        # Precision, recall\n","        metrics[i, 1] = np.divide(n_intersection, n_pred, out=np.zeros_like(n_intersection, dtype='float'),\n","                                  where=n_pred > 0).sum() \n","        metrics[i, 2] = np.divide(n_intersection, n_gt, out=np.zeros_like(n_gt, dtype='float'), where=n_gt > 0).sum()\n","\n","        if ic_arr is not None:\n","            # Terms subsets\n","            remaining = np.logical_and(np.logical_not(p), g)  # FN --> not predicted but in the ground truth\n","            mis = np.logical_and(p, np.logical_not(g))  # FP --> predicted but not in the ground truth\n","\n","            # Weighted precision, recall\n","            #wn_pred = (p * ic_arr[toi]).sum(axis=1)\n","            wn_pred=np.dot(p , ic_arr[toi])\n","            #wn_intersection = (intersection * ic_arr[toi]).sum(axis=1)\n","            wn_intersection = np.dot(intersection ,ic_arr[toi])\n","            \n","            #print(f\"Shape {wn_intersection.shape},{wn_pred.shape},  {n_intersection.shape},{n_pred.shape}\")\n","            metrics[i, 3] = np.divide(wn_intersection.reshape(-1), wn_pred.reshape(-1), out=np.zeros_like(n_intersection, dtype='float'),\n","                                      where=wn_pred.reshape(-1) > 0).sum() \n","            metrics[i, 4] = np.divide(wn_intersection.reshape(-1), wn_gt.reshape(-1), out=np.zeros_like(n_intersection, dtype='float'),\n","                                      where=wn_gt.reshape(-1) > 0).sum() \n","\n","            # Misinformation, remaining uncertainty\n","            #metrics[i, 5] = (remaining * ic_arr[toi]).sum(axis=1).sum()\n","            metrics[i, 5] = np.dot(remaining , ic_arr[toi]).sum()\n","            #metrics[i, 6] = (mis * ic_arr[toi]).sum(axis=1).sum()\n","            metrics[i, 6] = np.dot(mis , ic_arr[toi]).sum()\n","    return metrics\n","# computes the f metric for each precision and recall in the input arrays\n","def compute_f(pr, rc):\n","    n = 2 * pr * rc\n","    d = pr + rc\n","    return np.divide(n, d, out=np.zeros_like(n, dtype=float), where=d != 0)\n","\n","\n","def compute_s(ru, mi):\n","    return np.sqrt(ru**2 + mi**2)\n","    # return np.where(np.isnan(ru), mi, np.sqrt(ru + np.nan_to_num(mi)))\n","\n","def compute_metrics(pred, gt, toi, tau_arr, ic_arr=None, n_cpu=0):\n","    \"\"\"\n","    Takes the prediction and the ground truth and for each threshold in tau_arr\n","    calculates the confusion matrix and returns the coverage,\n","    precision, recall, remaining uncertainty and misinformation.\n","    Toi is the list of terms (indexes) to be considered\n","    \"\"\"\n","    g = gt[:, toi]\n","    n_gt = g.sum(axis=1)\n","    wn_gt = None\n","    if ic_arr is not None:\n","        #wn_gt = (g * ic_arr[toi]).sum(axis=1)\n","        wn_gt =np.dot(g,ic_arr[toi])\n","\n","    \n","    \n","    \n","    arg_lists = [[tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr] for tau_arr in np.array_split(tau_arr, 1)]\n","    #print(f\" Nº  de cpus {n_cpu}, nº de args {len(   arg_lists)}\")\n","    #with mp.Pool(processes=n_cpu) as pool:\n","    #    metrics = np.concatenate(pool.starmap(compute_metrics_, arg_lists), axis=0)\n","\n","    results = []\n","    for args in arg_lists:\n","        result = compute_metrics_(*args)\n","        results.append(result)\n","\n","    metrics = np.concatenate(results, axis=0)\n","    metrics=pd.DataFrame(metrics, columns=[\"cov\", \"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"])\n","    for column in [\"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"]:\n","        metrics[column] = np.divide(metrics[column], metrics[\"cov\"], out=np.zeros_like(metrics[column], dtype='float'), where=metrics[\"cov\"] > 0)\n","\n","    ne = np.full(len(tau_arr), gt.shape[0])\n","    metrics['ns'] = [\"\"] * len(tau_arr)\n","    metrics['tau'] = tau_arr\n","    metrics['cov'] = np.divide(metrics['cov'], ne, out=np.zeros_like(metrics['cov'], dtype='float'), where=ne > 0)\n","    metrics['f'] = compute_f(metrics['pr'], metrics['rc'])\n","    metrics['wf'] = compute_f(metrics['wpr'], metrics['wrc'])\n","    metrics['s'] = compute_s(metrics['ru'], metrics['mi'])\n","\n","    return metrics\n","\n","# Return a mask for all the predictions (matrix) >= tau\n","def solidify_prediction(pred, tau):\n","    return pred >= tau\n","def custom_f1_score(y_validate,y_pred):\n","            max_scores={}\n","            max_umbrales={}\n","            scores_dict={}\n","            for aspect,indice in aspects.items():\n","                scores_dict[aspect]={}\n","                max_score=0\n","                max_umbral=0\n","                for umbral in np.arange(0.05,0.5,0.05):\n","\n","                    y_pred_=(y_pred>umbral).astype(int)[:,indice]\n","                    scores_sums=y_pred_.sum(axis=1)\n","            \n","                    n=(scores_sums>0.5).sum()\n","                    #print(f\"n = {n} Secuencias con algun score 1 {n/len(y_pred_)}\")\n","                    y_validate_=y_validate.values[:,indice]\n","\n","                    y_validate_sums=y_validate_.sum(axis=1)\n","                    w_use_=w_use[indice,:]\n","\n","                    f1_scores = f1_score(y_validate_, y_pred_, average=None)\n","                    \n","                    t_p=y_pred_*y_validate_\n","                    \n","                    numerator=(np.dot(t_p,w_use_))\n","                    precision_denom=np.dot(y_pred_,w_use_)\n","                    precision_denom[precision_denom==0]=1\n","                    weighted_precision= np.sum(   numerator /  precision_denom) / n\n","\n","\n","                    \n","                    recall_denom= np.dot(y_validate_,w_use_)\n","                    recall_denom[recall_denom==0]=1\n","\n","                \n","                    weighted_recall=    np.sum(numerator /   recall_denom) / len(y_pred_)\n","                    average_f1_score = 2*weighted_precision* weighted_recall/( weighted_recall+ weighted_precision)\n","            \n","                    scores_dict[aspect][umbral]= average_f1_score \n","                # Calcula el F1-score promedio\n","                #average_f1_score = np.mean(f1_scores)\n","                    if  average_f1_score > max_score:\n","                        max_score=average_f1_score \n","                        max_umbral=umbral\n","                max_scores[aspect]= max_score\n","                max_umbrales[aspect]=umbral\n","            \n","            max_score=np.mean(list(max_scores.values()))\n","            print(f\"Scores {scores_dict}\")\n","            # Print the F1 score\n","            print('F1 Score:', max_score)\n","\n","class PredictionFnCallback(tf.keras.callbacks.Callback):\n","    \n","    def __init__(self,X_validate, y_validate, model=None, verbose=0):\n","        \n","               if not model is None: self.model = model\n","               self.verbose = verbose\n","               self.X_validate = X_validate\n","               self.y_validate = y_validate\n","               self.num_classes=num_classes\n","            \n","    def on_epoch_end(self, epoch, logs=None):\n","       \n","        y_pred=(self.model.predict(self.X_validate))\n","        #custom_f1_score(y_validate,y_pred)\n","        ms=[]\n","        for aspect,indice in aspects.items():\n","            ms_=(compute_metrics(y_pred, self.y_validate.values, indice, np.arange(0.1,0.5,0.05), ic_arr=w_use, n_cpu=0))\n","            ms.append(ms_.wf.max())\n","        print(np.mean(ms))\n","            \n","\n","def custom_f1_score(y_validate,y_pred):\n","        #custom_f1_score(y_validate,y_pred)\n","        ms=[]\n","        for aspect,indice in aspects.items():\n","            ms_=(compute_metrics(y_pred, y_validate.values, indice, np.arange(0.1,0.5,0.05), ic_arr=w_use, n_cpu=0))\n","            ms.append(ms_.wf.max())\n","        print(np.mean(ms))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-07-09T20:45:00.214915Z","iopub.status.busy":"2023-07-09T20:45:00.214418Z","iopub.status.idle":"2023-07-09T20:45:00.243403Z","shell.execute_reply":"2023-07-09T20:45:00.241880Z","shell.execute_reply.started":"2023-07-09T20:45:00.214873Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","from sklearn.model_selection import KFold\n","n_splits = 5\n","\n","# Crear una instancia de KFold\n","kfold = KFold(n_splits=n_splits)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2023-07-09T20:45:00.246122Z","iopub.status.busy":"2023-07-09T20:45:00.245637Z","iopub.status.idle":"2023-07-09T22:35:13.417463Z","shell.execute_reply":"2023-07-09T22:35:13.415346Z","shell.execute_reply.started":"2023-07-09T20:45:00.246080Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[],"source":["models = []\n","\n","EPOCH_LOAD=\"10\"\n","VALIDATE = True\n","predictions_t=None\n","if TRAIN:\n","    for fold,(train_indice, test_indice) in enumerate(kfold.split(range(len(X)))):\n","    \n","        checkpoint_path = PATHS[\"CHECKPOINT\"][config.env].format(fold) + '/modelo-{epoch:02d}'\n","        checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","            filepath=checkpoint_path,\n","            save_weights_only=False,\n","            save_freq='epoch',\n","\n","        )\n","        if model_name==\"cn\":\n","            model = CNN1D( num_classes)\n","        elif model_name==\"bp\":\n","            model =MultiLayerPerceptron(num_classes)\n","        optimizer = tf.keras.optimizers.Adam()\n","        loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","\n","        model.compile(optimizer=optimizer,loss=loss_fn,metrics=['binary_accuracy', tf.keras.metrics.AUC()])\n","        model.fit(x=X[train_indice],y=y.iloc[train_indice],epochs=10,batch_size=32, callbacks=[PredictionFnCallback(X[test_indice],y.iloc[test_indice]),checkpoint_callback],)\n","        models.append(model)\n","        if VALIDATE:\n","            loaded_model = tf.keras.models.load_model( PATHS[\"CHECKPOINT\"][config.env].format(fold)+f'/modelo-{EPOCH_LOAD}')\n","            # Use the loaded model for predictions or further operations\n","            predictions = loaded_model.predict(X[test_indice])\n","\n","            custom_f1_score(y.iloc[test_indice],np.round(predictions,3) )\n","\n","            print(np.round(predictions,3)[:10])\n","            predictions=np.round(predictions,3)\n","            predictions=pd.DataFrame(predictions,index=y.iloc[test_indice].index,columns=y.iloc[test_indice].columns,)\n","            predictions=predictions.stack()\n","            predictions=predictions[predictions>0]\n","            print(predictions.shape)\n","            predictions_t=pd.concat([predictions_t,predictions])\n","            predictions.to_csv(PATHS[\"PREDICTIONS\"][config.env].format(fold),header=False, index=True, sep=\"\\t\")\n","            labels.loc[labels.EntryID.isin(y.iloc[test_indice].index)].to_csv(PATHS[\"LABELS\"][config.env].format(fold),sep=\"\\t\",index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-09T22:35:13.419220Z","iopub.status.idle":"2023-07-09T22:35:13.419721Z","shell.execute_reply":"2023-07-09T22:35:13.419507Z","shell.execute_reply.started":"2023-07-09T22:35:13.419483Z"},"trusted":true},"outputs":[],"source":["def get_dataset( datatype, embeddings_source):\n","        ids_= np.load(f\"{PATH}\" + embeds_map[\"T5\"] + \"/\" + datatype + \"_ids.npy\")\n","        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n","            embeds = np.load(f\"{PATH}\" + embeds_map[embeddings_source] + \"/\" + datatype + \"_embeddings.npy\")\n","            ids = np.load(f\"{PATH}\" + embeds_map[embeddings_source] + \"/\" + datatype + \"_ids.npy\")\n","            dic_ids={e:i for i,e in enumerate(ids)}\n","            indices=[dic_ids[e] for e in ids_]\n","            embeds=np.array([embeds[i] for i in indices])\n","        \n","        if embeddings_source == \"T5\":\n","            embeds = np.load(f\"{PATH}\"  + embeds_map[embeddings_source] + \"/\" + datatype + \"_embeds.npy\")\n","            ids = np.load(f\"{PATH}\"  + embeds_map[embeddings_source] + \"/\" + datatype + \"_ids.npy\")\n","            \n","\n","        if datatype==\"train\":\n","            df_labels=pd.read_pickle(PATH_LABELS)\n","            \n","            return embeds,ids,df_labels.loc[ids]\n","        return embeds,ids_\n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-09T22:35:13.431444Z","iopub.status.idle":"2023-07-09T22:35:13.432592Z","shell.execute_reply":"2023-07-09T22:35:13.432273Z","shell.execute_reply.started":"2023-07-09T22:35:13.432236Z"},"trusted":true},"outputs":[],"source":["X_test1, ids_test= get_dataset( \"test\", embeddings_source1)\n","\n","X_test2, _= get_dataset( \"test\", embeddings_source2)\n","if X_test2.shape[1]>  X_test1.shape[1]:\n","        X_test1 = np.pad(X_test1,((0,0),(0,X_test2.shape[1]-X_test1.shape[1])))\n","X_test=np.concatenate([np.expand_dims(X_test1,-1),np.expand_dims(X_test2,-1)],axis=-1)\n","\n","\n","\n","predictions_test_df_t =None\n","for fold in range(n_splits):\n","    loaded_model = tf.keras.models.load_model(f'models_{fold}/modelo-10')\n","\n","    predictions_test = np.round(loaded_model.predict(X_test),3)\n","    print(predictions_test.shape)\n","    print(predictions_test[:10])\n","\n","\n","\n","    print(len(ids_test),len(y.columns))\n","    predictions_test_df=pd.DataFrame(predictions_test,index=ids_test,columns=y.columns,)\n","    print((predictions_test_df==0).sum().max())\n","\n","\n","\n","\n","    predictions_test_df=predictions_test_df.stack()\n","    \n","    if predictions_test_df_t is None:\n","        predictions_test_df_t= predictions_test_df\n","    else:\n","        predictions_test_df_t=     predictions_test_df_t+ predictions_test_df\n","        \n","        \n","            \n","predictions_test_df_t=predictions_test_df_t/n_splits\n","predictions_test_df=predictions_test_df_t\n","print(predictions_test_df.shape)\n","\n","\n","predictions_test_df=predictions_test_df[predictions_test_df>0]\n","print(predictions_test_df.shape)\n","\n","predictions_test_df.to_csv(\"submission.tsv\",header=False, index=True, sep=\"\\t\")\n","\n","\n","df_submission = pd.read_csv(\"submission.tsv\",header=None, sep=\"\\t\")\n","display(df_submission)\n","                            "]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-07-09T22:35:13.434314Z","iopub.status.idle":"2023-07-09T22:35:13.435337Z","shell.execute_reply":"2023-07-09T22:35:13.435042Z","shell.execute_reply.started":"2023-07-09T22:35:13.435009Z"},"trusted":true},"outputs":[],"source":["predictions_test_df"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
