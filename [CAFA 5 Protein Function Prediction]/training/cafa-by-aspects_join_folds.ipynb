{"cells":[{"cell_type":"code","execution_count":25,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:40:45.488939Z","iopub.status.busy":"2023-07-13T16:40:45.488486Z","iopub.status.idle":"2023-07-13T16:40:53.814257Z","shell.execute_reply":"2023-07-13T16:40:53.813338Z","shell.execute_reply.started":"2023-07-13T16:40:45.488908Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["1.5.3\n","TensorFlow is not using GPU\n"]}],"source":["import pandas as pd\n","print(pd.__version__)\n","import numpy as np\n","\n","# UTILITARIES\n","import numpy as np\n","from tqdm import tqdm\n","import time\n","import matplotlib.pyplot as plt\n","plt.style.use('ggplot')\n","import tensorflow as tf\n","import os\n","\n","# Check if GPU is available and TensorFlow is using it\n","if tf.test.is_gpu_available():\n","    print('TensorFlow is using GPU')\n","else:\n","    print('TensorFlow is not using GPU')"]},{"cell_type":"code","execution_count":26,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:40:53.816869Z","iopub.status.busy":"2023-07-13T16:40:53.816171Z","iopub.status.idle":"2023-07-13T16:40:53.828585Z","shell.execute_reply":"2023-07-13T16:40:53.827788Z","shell.execute_reply.started":"2023-07-13T16:40:53.816837Z"},"trusted":true},"outputs":[],"source":["env_=\"local\"\n","PATH = \"/kaggle/input/\"\n","PATH=\"../data/\"\n","if env_==\"COLAB_INIT\":\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","  ! pip install kaggle\n","  ! mkdir ~/.kaggle\n","  os.getcwd()\n","  ! pip install Bio\n","\n","\n","MAIN_DIR = f\"{PATH}cafa-5-protein-function-prediction\"\n","\n","class config:\n","    train_sequences_path = MAIN_DIR  + \"/Train/train_sequences.fasta\"\n","    train_labels_path = MAIN_DIR + \"/Train/train_terms.tsv\"\n","    test_sequences_path = MAIN_DIR + \"/Test (Targets)/testsuperset.fasta\"\n","    \n","    num_labels = 1500\n","    n_epochs = 5\n","    batch_size = 128\n","    lr = 0.001\n","\n","    env = env_\n","\n","    \n","TRAIN = False\n","embeddings_source= \"T5\"\n","\n","PATH_DATAFRAMES=PATH_LABELS=\"../data/data_train\"\n","PATH_DATAFRAMES_2D=\"data/data_train_embeds_2d\"\n","if env_==\"kaggle\":\n","    PATH_DATAFRAMES=\"/kaggle/input/dataframes-train-cafa/data_train\"\n","    PATH_DATAFRAMES_2D=\"/kaggle/input/dataframes-train-cafa/data_train_embeds_2d\"\n","    PATH_LABELS=\"/kaggle/input/dataframes-train-cafa/\""]},{"cell_type":"code","execution_count":27,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:40:53.830527Z","iopub.status.busy":"2023-07-13T16:40:53.829699Z","iopub.status.idle":"2023-07-13T16:40:54.127834Z","shell.execute_reply":"2023-07-13T16:40:54.126536Z","shell.execute_reply.started":"2023-07-13T16:40:53.830498Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>The Protein ID</th>\n","      <th>The Gene Ontology term (GO) ID</th>\n","      <th>Predicted link probability that GO appear in Protein</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>A0A0A0MRZ7</td>\n","      <td>GO:0000001</td>\n","      <td>0.123</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>A0A0A0MRZ7</td>\n","      <td>GO:0000002</td>\n","      <td>0.123</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>A0A0A0MRZ8</td>\n","      <td>GO:0000001</td>\n","      <td>0.123</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>A0A0A0MRZ8</td>\n","      <td>GO:0000002</td>\n","      <td>0.123</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>A0A0A0MRZ9</td>\n","      <td>GO:0000001</td>\n","      <td>0.123</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  The Protein ID The Gene Ontology term (GO) ID  \\\n","0     A0A0A0MRZ7                     GO:0000001   \n","1     A0A0A0MRZ7                     GO:0000002   \n","2     A0A0A0MRZ8                     GO:0000001   \n","3     A0A0A0MRZ8                     GO:0000002   \n","4     A0A0A0MRZ9                     GO:0000001   \n","\n","   Predicted link probability that GO appear in Protein  \n","0                                              0.123     \n","1                                              0.123     \n","2                                              0.123     \n","3                                              0.123     \n","4                                              0.123     "]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","sub = pd.read_csv(f\"{PATH}cafa-5-protein-function-prediction/sample_submission.tsv\", sep= \"\\t\", header = None)\n","sub.columns = [\"The Protein ID\", \"The Gene Ontology term (GO) ID\", \"Predicted link probability that GO appear in Protein\"]\n","sub.head(5)"]},{"cell_type":"code","execution_count":28,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:40:54.129914Z","iopub.status.busy":"2023-07-13T16:40:54.129463Z","iopub.status.idle":"2023-07-13T16:40:54.136797Z","shell.execute_reply":"2023-07-13T16:40:54.135110Z","shell.execute_reply.started":"2023-07-13T16:40:54.129886Z"},"trusted":true},"outputs":[],"source":["\n","PATHS = {\n","    \"PATH\": PATH, \n","    \"PATH_LABELS\": PATH_LABELS,\n","    \"PATH_DATAFRAMES_2D\": PATH_DATAFRAMES_2D,\n","    \"CHECKPOINT\": {\"local\":'../results/models_folds_aspect/models_{}_{}', \"kaggle\": '/kaggle/working/models_{}_{}'},\n","    \"PREDICTIONS\": {\"local\":'../results/preds_cafa_embeddings_bp/preds_fold{}.tsv', \"kaggle\": 'preds_fold{}.tsv'},\n","    \"LABELS\": {\"local\":'../results/preds_cafa_embeddings_bp_aspect/labels_used_fold{}.tsv', \"kaggle\": 'labels_used_fold{}.tsv'},\n","}"]},{"cell_type":"code","execution_count":29,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:40:54.139817Z","iopub.status.busy":"2023-07-13T16:40:54.139486Z","iopub.status.idle":"2023-07-13T16:41:00.556271Z","shell.execute_reply":"2023-07-13T16:41:00.555204Z","shell.execute_reply.started":"2023-07-13T16:40:54.139789Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Loading train set ProtBERT Embeddings...\n","Total Nb of Elements :  142246\n","Total Nb of Elements :  141865\n"]}],"source":["from Bio import SeqIO\n","print(\"Loading train set ProtBERT Embeddings...\")\n","fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n","print(\"Total Nb of Elements : \", len(list(fasta_train)))\n","fasta_test= SeqIO.parse(config.test_sequences_path, \"fasta\")\n","print(\"Total Nb of Elements : \", len(list(fasta_test)))"]},{"cell_type":"code","execution_count":30,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:00.558393Z","iopub.status.busy":"2023-07-13T16:41:00.557764Z","iopub.status.idle":"2023-07-13T16:41:00.613680Z","shell.execute_reply":"2023-07-13T16:41:00.612361Z","shell.execute_reply.started":"2023-07-13T16:41:00.558353Z"},"trusted":true},"outputs":[],"source":["weights=pd.read_csv(f\"{PATH}cafa-5-protein-function-prediction/IA.txt\",sep=\"\\t\",header=None,index_col=0).rename(columns={1:\"weight\"})\n"]},{"cell_type":"code","execution_count":31,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:00.615446Z","iopub.status.busy":"2023-07-13T16:41:00.614946Z","iopub.status.idle":"2023-07-13T16:41:03.914681Z","shell.execute_reply":"2023-07-13T16:41:03.913744Z","shell.execute_reply.started":"2023-07-13T16:41:00.615410Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(5363863, 3)"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":[" labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n"," labels.shape"]},{"cell_type":"code","execution_count":32,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:03.916193Z","iopub.status.busy":"2023-07-13T16:41:03.915888Z","iopub.status.idle":"2023-07-13T16:41:04.634886Z","shell.execute_reply":"2023-07-13T16:41:04.633834Z","shell.execute_reply.started":"2023-07-13T16:41:03.916168Z"},"trusted":true},"outputs":[{"data":{"text/plain":["(4861356, 3)"]},"execution_count":32,"metadata":{},"output_type":"execute_result"}],"source":[" labels= labels.loc[~labels.term.isin(list(weights[weights.weight==0].index))]\n"," labels.shape"]},{"cell_type":"code","execution_count":33,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:04.636601Z","iopub.status.busy":"2023-07-13T16:41:04.636187Z","iopub.status.idle":"2023-07-13T16:41:04.641957Z","shell.execute_reply":"2023-07-13T16:41:04.640962Z","shell.execute_reply.started":"2023-07-13T16:41:04.636566Z"},"trusted":true},"outputs":[],"source":["# Directories for the different embedding vectors : \n","embeds_map = {\n","    \"T5\" : \"my-t5embeds\",\n","    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n","    \"EMS2\" : \"cafa-5-ems-2-embeddings-numpy\"\n","}\n","\n","# Length of the different embedding vectors :\n","embeds_dim = {\n","    \"T5\" : 1024,\n","    \"ProtBERT\" : 1024,\n","    \"EMS2\" : 1280\n","}"]},{"cell_type":"code","execution_count":34,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:04.643581Z","iopub.status.busy":"2023-07-13T16:41:04.643298Z","iopub.status.idle":"2023-07-13T16:41:04.661430Z","shell.execute_reply":"2023-07-13T16:41:04.660035Z","shell.execute_reply.started":"2023-07-13T16:41:04.643557Z"},"trusted":true},"outputs":[{"data":{"text/plain":["'\\nPredictionFnCallback is used for:\\n1. Loading validation data\\n2. FOGModel data preparation\\n3. Prediction\\n4. Scoring and save\\n\\n'"]},"execution_count":34,"metadata":{},"output_type":"execute_result"}],"source":[" LEARNING_RATE=0.001\n"," WARMUP_STEPS=7\n"," class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n","    def __init__(self, initial_lr, warmup_steps=1):\n","        super(CustomSchedule, self).__init__()\n","\n","        self.initial_lr = tf.cast(initial_lr, tf.float32)\n","        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n","\n","    def __call__(self, step):\n","        step = tf.cast(step, tf.float32)\n","        return tf.math.minimum(self.initial_lr, self.initial_lr * (step/self.warmup_steps))  \n","    \n","    def get_config(self):\n","        return {\"initial_learning_rate\": self.initial_lr}\n","    \n","\n","'''\n","PredictionFnCallback is used for:\n","1. Loading validation data\n","2. FOGModel data preparation\n","3. Prediction\n","4. Scoring and save\n","\n","'''"]},{"cell_type":"code","execution_count":35,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:04.663433Z","iopub.status.busy":"2023-07-13T16:41:04.663107Z","iopub.status.idle":"2023-07-13T16:41:04.692362Z","shell.execute_reply":"2023-07-13T16:41:04.691134Z","shell.execute_reply.started":"2023-07-13T16:41:04.663408Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","#models_dp_v0\n","class MultiLayerPerceptron_v0(tf.keras.Model):\n","\n","    def __init__(self, num_classes):\n","        super(MultiLayerPerceptron, self).__init__()\n","                \n","         \n","        self.linear1 = tf.keras.layers.Dense(256)\n","        #self.activation1 = tf.keras.layers.ReLU()\n","        self.linear2 = tf.keras.layers.Dense(128)\n","        #self.activation2 = tf.keras.layers.ReLU()\n","        self.linear2 = tf.keras.layers.Dense(56)\n","        \n","        self.linear3 = tf.keras.layers.Dense(num_classes,activation=\"sigmoid\")\n","\n","    def call(self, x):\n","\n","        x = self.linear1(x)\n","        #x = self.activation1(x)\n","        x = self.linear2(x)\n","        # x = self.activation2(x)\n","        x = self.linear3(x)\n","        return x\n","import tensorflow as tf\n","\n","\n","\n","#models_dp_v1\n","class ResidualBlock(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(ResidualBlock, self).__init__()\n","        self.fc1 = tf.keras.layers.Dense(units, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(units, activation='relu')\n","\n","    def call(self, x):\n","        residual = x\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = x + residual\n","        return x\n","\n","class MultiLayerPerceptron(tf.keras.Model):\n","    def __init__(self, num_classes):\n","        super(MultiLayerPerceptron, self).__init__()\n","        self.fc1 = tf.keras.layers.Dense(128, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(56, activation='relu')\n","        self.res_block1 = ResidualBlock(56)\n","        self.res_block2 = ResidualBlock(56)\n","        self.fc3 = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n","\n","    def call(self, x):\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = self.res_block1(x)\n","        x = self.res_block2(x)\n","        x = self.fc3(x)\n","        return x\n","\n","\n","#models_dp_v2\n","class ResidualBlock_v2(tf.keras.layers.Layer):\n","    def __init__(self, units):\n","        super(ResidualBlock, self).__init__()\n","        self.fc1 = tf.keras.layers.Dense(units, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(units, activation='relu')\n","\n","    def call(self, x):\n","        residual = x\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        x = x + residual\n","        return x\n","\n","class MultiLayerPerceptron_v2(tf.keras.Model):\n","    def __init__(self, num_classes):\n","        super(MultiLayerPerceptron, self).__init__()\n","        self.res_block1 = ResidualBlock(1024)\n","        self.fc1 = tf.keras.layers.Dense(128, activation='relu')\n","        self.res_block2 = ResidualBlock(512)\n","        self.fc2 = tf.keras.layers.Dense(256, activation='relu')\n","        self.res_block3 = ResidualBlock(256)\n","        self.fc3 = tf.keras.layers.Dense(128, activation='relu')\n","        self.fc4 = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n","\n","    def call(self, x):\n","        # x = self.res_block1(x)\n","        x = self.fc1(x)\n","        # x = self.res_block2(x)\n","        # x = self.fc2(x)\n","        # x = self.res_block3(x)\n","        # x = self.fc3(x)\n","        x = self.fc4(x)\n","        return x\n","\n","\n","\n","\n","#models_cnn_v0\n","class CNN1D(tf.keras.Model):  #v0\n","    def __init__(self, num_classes):\n","        super(CNN1D, self).__init__()\n","        # (batch_size, channels, embed_size)\n","        self.conv1 = tf.keras.layers.Conv1D(filters=7, kernel_size=3, dilation_rate=1, padding='same', activation='relu')\n","        # (batch_size, 3, embed_size)\n","        self.pool1 = tf.keras.layers.MaxPool1D(pool_size=2, strides=2)\n","        # (batch_size, 3, embed_size/2 = 512)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=15, kernel_size=3, dilation_rate=1, padding='same', activation='relu')\n","        # (batch_size, 8, embed_size/2 = 512)\n","        self.pool2 = tf.keras.layers.MaxPool1D(pool_size=2, strides=2)\n","        \n","        self.flatten=tf.keras.layers.Flatten()\n","        # (batch_size, 8, embed_size/4 = 256)\n","        self.fc1 = tf.keras.layers.Dense(units=128, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(units=num_classes,activation=\"sigmoid\")\n","\n","    def call(self, x):\n","        x = tf.reshape(x, [-1, tf.shape(x)[1],1])\n","        x=self.conv1(x)\n","        x = self.pool1(x)\n","        x = self.pool2(self.conv2(x))\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","#models_cnn_v0\n","class CNN1D(tf.keras.Model):  #v0\n","    def __init__(self, num_classes):\n","        super(CNN1D, self).__init__()\n","        # (batch_size, channels, embed_size)\n","        self.conv1 = tf.keras.layers.Conv1D(filters=7, kernel_size=3, dilation_rate=1, padding='same', activation='relu')\n","        # (batch_size, 3, embed_size)\n","        self.pool1 = tf.keras.layers.MaxPool1D(pool_size=2, strides=2)\n","        # (batch_size, 3, embed_size/2 = 512)\n","        self.conv2 = tf.keras.layers.Conv1D(filters=15, kernel_size=3, dilation_rate=1, padding='same', activation='relu')\n","        # (batch_size, 8, embed_size/2 = 512)\n","        self.pool2 = tf.keras.layers.MaxPool1D(pool_size=2, strides=2)\n","        \n","        self.flatten=tf.keras.layers.Flatten()\n","        # (batch_size, 8, embed_size/4 = 256)\n","        self.fc1 = tf.keras.layers.Dense(units=128, activation='relu')\n","        self.fc2 = tf.keras.layers.Dense(units=num_classes,activation=\"sigmoid\")\n","\n","    def call(self, x):\n","        x = tf.reshape(x, [-1, tf.shape(x)[1],1])\n","        x=self.conv1(x)\n","        x = self.pool1(x)\n","        x = self.pool2(self.conv2(x))\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.fc2(x)\n","        return x\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# 7. Train the Model"]},{"cell_type":"code","execution_count":36,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:04.693991Z","iopub.status.busy":"2023-07-13T16:41:04.693641Z","iopub.status.idle":"2023-07-13T16:41:33.610748Z","shell.execute_reply":"2023-07-13T16:41:33.609666Z","shell.execute_reply.started":"2023-07-13T16:41:04.693958Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","if TRAIN:\n","    #X_train = np.load(os.path.join(PATH_DATAFRAMES,f\"embeds_train_{embeddings_source}.npy\"))\n","    #X_validate = np.load(os.path.join(PATH_DATAFRAMES,f\"embeds_test_{embeddings_source}.npy\"))\n","    #y_train = pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_train_{   config. num_labels}.pkl\"))\n","    #y_validate = pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_test_{   config. num_labels}.pkl\"))\n","    X=np.load(os.path.join(PATH_DATAFRAMES,f\"embeds_{embeddings_source}.npy\")).astype(np. float32)\n","    y=pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_{   config. num_labels}.pkl\")).astype(np. float32)\n","\n","    num_classes=y.shape[1]\n","    print(f\" Num classes {num_classes}\")\n","    print(X.shape,y.shape)\n","    w_use=weights.loc[y.columns].values\n","    print(w_use.shape)\n","    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n","    terms=labels[[\"term\",\"aspect\"]].drop_duplicates().set_index(\"term\").loc[y.columns]\n","    aspects={}\n","    for aspect,df_terms in terms.groupby(\"aspect\"):\n","        terms_index=list(df_terms.index.values)\n","        display(df_terms.head(5))\n","        indices = list(np.where(np.isin(y.columns,    terms_index))[0])\n","        aspects[aspect]=indices\n","    print(terms.shape)\n"]},{"cell_type":"code","execution_count":37,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:33.612982Z","iopub.status.busy":"2023-07-13T16:41:33.612524Z","iopub.status.idle":"2023-07-13T16:41:34.281166Z","shell.execute_reply":"2023-07-13T16:41:34.280053Z","shell.execute_reply.started":"2023-07-13T16:41:33.612943Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","\n","from sklearn.model_selection import KFold\n","n_splits = 5\n","\n","# Crear una instancia de KFold\n","kfold = KFold(n_splits=n_splits)\n","\n"]},{"cell_type":"code","execution_count":38,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:34.284794Z","iopub.status.busy":"2023-07-13T16:41:34.284452Z","iopub.status.idle":"2023-07-13T16:41:34.319029Z","shell.execute_reply":"2023-07-13T16:41:34.318043Z","shell.execute_reply.started":"2023-07-13T16:41:34.284764Z"},"jupyter":{"source_hidden":true},"trusted":true},"outputs":[],"source":["from sklearn.metrics import f1_score\n","\n","def compute_metrics_(tau_arr, g, pred, toi, n_gt, wn_gt=None, ic_arr=None):\n","\n","    metrics = np.zeros((len(tau_arr), 7), dtype='float')  # cov, pr, rc, wpr, wrc, ru, mi\n","\n","    for i, tau in enumerate(tau_arr):\n","\n","        p = solidify_prediction(pred[:, toi], tau)\n","\n","        # number of proteins with at least one term predicted with score >= tau\n","        metrics[i, 0] = (p.sum(axis=1) > 0).sum()\n","\n","        # Terms subsets\n","        intersection = np.logical_and(p, g)  # TP\n","\n","        \n","\n","        # Subsets size\n","        n_pred = p.sum(axis=1)\n","        n_intersection = intersection.sum(axis=1)\n","\n","        # Precision, recall\n","        metrics[i, 1] = np.divide(n_intersection, n_pred, out=np.zeros_like(n_intersection, dtype='float'),\n","                                  where=n_pred > 0).sum() \n","        metrics[i, 2] = np.divide(n_intersection, n_gt, out=np.zeros_like(n_gt, dtype='float'), where=n_gt > 0).sum()\n","\n","        if ic_arr is not None:\n","            # Terms subsets\n","            remaining = np.logical_and(np.logical_not(p), g)  # FN --> not predicted but in the ground truth\n","            mis = np.logical_and(p, np.logical_not(g))  # FP --> predicted but not in the ground truth\n","\n","            # Weighted precision, recall\n","            #wn_pred = (p * ic_arr[toi]).sum(axis=1)\n","            wn_pred=np.dot(p , ic_arr[toi])\n","            #wn_intersection = (intersection * ic_arr[toi]).sum(axis=1)\n","            wn_intersection = np.dot(intersection ,ic_arr[toi])\n","            \n","            #print(f\"Shape {wn_intersection.shape},{wn_pred.shape},  {n_intersection.shape},{n_pred.shape}\")\n","            metrics[i, 3] = np.divide(wn_intersection.reshape(-1), wn_pred.reshape(-1), out=np.zeros_like(n_intersection, dtype='float'),\n","                                      where=wn_pred.reshape(-1) > 0).sum() \n","            metrics[i, 4] = np.divide(wn_intersection.reshape(-1), wn_gt.reshape(-1), out=np.zeros_like(n_intersection, dtype='float'),\n","                                      where=wn_gt.reshape(-1) > 0).sum() \n","\n","            # Misinformation, remaining uncertainty\n","            #metrics[i, 5] = (remaining * ic_arr[toi]).sum(axis=1).sum()\n","            metrics[i, 5] = np.dot(remaining , ic_arr[toi]).sum()\n","            #metrics[i, 6] = (mis * ic_arr[toi]).sum(axis=1).sum()\n","            metrics[i, 6] = np.dot(mis , ic_arr[toi]).sum()\n","    return metrics\n","# computes the f metric for each precision and recall in the input arrays\n","def compute_f(pr, rc):\n","    n = 2 * pr * rc\n","    d = pr + rc\n","    return np.divide(n, d, out=np.zeros_like(n, dtype=float), where=d != 0)\n","\n","\n","def compute_s(ru, mi):\n","    return np.sqrt(ru**2 + mi**2)\n","    # return np.where(np.isnan(ru), mi, np.sqrt(ru + np.nan_to_num(mi)))\n","\n","def compute_metrics(pred, gt, toi, tau_arr, ic_arr=None, n_cpu=0):\n","    \"\"\"\n","    Takes the prediction and the ground truth and for each threshold in tau_arr\n","    calculates the confusion matrix and returns the coverage,\n","    precision, recall, remaining uncertainty and misinformation.\n","    Toi is the list of terms (indexes) to be considered\n","    \"\"\"\n","    g = gt[:, toi]\n","    n_gt = g.sum(axis=1)\n","    wn_gt = None\n","    if ic_arr is not None:\n","        #wn_gt = (g * ic_arr[toi]).sum(axis=1)\n","        wn_gt =np.dot(g,ic_arr[toi])\n","\n","    \n","    \n","    \n","    arg_lists = [[tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr] for tau_arr in np.array_split(tau_arr, 1)]\n","    #print(f\" Nº  de cpus {n_cpu}, nº de args {len(   arg_lists)}\")\n","    #with mp.Pool(processes=n_cpu) as pool:\n","    #    metrics = np.concatenate(pool.starmap(compute_metrics_, arg_lists), axis=0)\n","\n","    results = []\n","    for args in arg_lists:\n","        result = compute_metrics_(*args)\n","        results.append(result)\n","\n","    metrics = np.concatenate(results, axis=0)\n","    metrics=pd.DataFrame(metrics, columns=[\"cov\", \"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"])\n","    for column in [\"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"]:\n","        metrics[column] = np.divide(metrics[column], metrics[\"cov\"], out=np.zeros_like(metrics[column], dtype='float'), where=metrics[\"cov\"] > 0)\n","\n","    ne = np.full(len(tau_arr), gt.shape[0])\n","    metrics['ns'] = [\"\"] * len(tau_arr)\n","    metrics['tau'] = tau_arr\n","    metrics['cov'] = np.divide(metrics['cov'], ne, out=np.zeros_like(metrics['cov'], dtype='float'), where=ne > 0)\n","    metrics['f'] = compute_f(metrics['pr'], metrics['rc'])\n","    metrics['wf'] = compute_f(metrics['wpr'], metrics['wrc'])\n","    metrics['s'] = compute_s(metrics['ru'], metrics['mi'])\n","\n","    return metrics\n","\n","# Return a mask for all the predictions (matrix) >= tau\n","def solidify_prediction(pred, tau):\n","    return pred >= tau\n","def custom_f1_score(y_validate,y_pred):\n","            max_scores={}\n","            max_umbrales={}\n","            scores_dict={}\n","            for aspect,indice in aspects.items():\n","                scores_dict[aspect]={}\n","                max_score=0\n","                max_umbral=0\n","                for umbral in np.arange(0.05,0.5,0.05):\n","\n","                    y_pred_=(y_pred>umbral).astype(int)[:,indice]\n","                    scores_sums=y_pred_.sum(axis=1)\n","            \n","                    n=(scores_sums>0.5).sum()\n","                    #print(f\"n = {n} Secuencias con algun score 1 {n/len(y_pred_)}\")\n","                    y_validate_=y_validate.values[:,indice]\n","\n","                    y_validate_sums=y_validate_.sum(axis=1)\n","                    w_use_=w_use[indice,:]\n","\n","                    f1_scores = f1_score(y_validate_, y_pred_, average=None)\n","                    \n","                    t_p=y_pred_*y_validate_\n","                    \n","                    numerator=(np.dot(t_p,w_use_))\n","                    precision_denom=np.dot(y_pred_,w_use_)\n","                    precision_denom[precision_denom==0]=1\n","                    weighted_precision= np.sum(   numerator /  precision_denom) / n\n","\n","\n","                    \n","                    recall_denom= np.dot(y_validate_,w_use_)\n","                    recall_denom[recall_denom==0]=1\n","\n","                \n","                    weighted_recall=    np.sum(numerator /   recall_denom) / len(y_pred_)\n","                    average_f1_score = 2*weighted_precision* weighted_recall/( weighted_recall+ weighted_precision)\n","            \n","                    scores_dict[aspect][umbral]= average_f1_score \n","                # Calcula el F1-score promedio\n","                #average_f1_score = np.mean(f1_scores)\n","                    if  average_f1_score > max_score:\n","                        max_score=average_f1_score \n","                        max_umbral=umbral\n","                max_scores[aspect]= max_score\n","                max_umbrales[aspect]=umbral\n","            \n","            max_score=np.mean(list(max_scores.values()))\n","            print(f\"Scores {scores_dict}\")\n","            # Print the F1 score\n","            print('F1 Score:', max_score)\n","\n","class PredictionFnCallback(tf.keras.callbacks.Callback):\n","    \n","    def __init__(self,X_validate,y_validate, model=None, verbose=0):\n","        \n","               if not model is None: self.model = model\n","               self.verbose = verbose\n","               self.X_validate = X_validate\n","               self.y_validate = y_validate\n","               self.num_classes=num_classes\n","            \n","    def on_epoch_end(self, epoch, logs=None):\n","       \n","        y_pred=(self.model.predict(self.X_validate))\n","        #custom_f1_score(y_validate,y_pred)\n","        ms = []\n","        w_use=weights.loc[self.y_validate.columns].values\n","        ms_=(compute_metrics(y_pred, self.y_validate.values, list(range(self.y_validate.shape[1])), np.arange(0.1,0.5,0.05), ic_arr=w_use, n_cpu=0))\n","        ms.append(ms_.wf.max())\n","        print(np.mean(ms))\n","            \n","\n","def custom_f1_score(y_validate,y_pred):\n","        #custom_f1_score(y_validate,y_pred)\n","        ms = []\n","        w_use=weights.loc[y_validate.columns].values\n","        ms_=(compute_metrics(y_pred,y_validate.values, list(range(y_validate.shape[1])), np.arange(0.1,0.5,0.05), ic_arr=w_use, n_cpu=0))\n","        ms.append(ms_.wf.max())\n","        print(np.mean(ms))\n","\n"]},{"cell_type":"code","execution_count":39,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:41:34.321377Z","iopub.status.busy":"2023-07-13T16:41:34.320964Z","iopub.status.idle":"2023-07-13T16:42:15.480121Z","shell.execute_reply":"2023-07-13T16:42:15.479250Z","shell.execute_reply.started":"2023-07-13T16:41:34.321339Z"},"trusted":true},"outputs":[],"source":["import tensorflow as tf\n","tf.config.list_physical_devices('GPU')\n","models = []\n","EPOCH_LOAD = \"12\"\n","VALIDATE =True\n","TRAIN =False\n","NUM_LABELS_dict= {        \"BPO\": 2000,\n","                              \"CCO\":2000,\n","                              \"MFO\": 1000}\n","ys={\n","        \"CCO\" : pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_{   NUM_LABELS_dict['CCO']}_CCO.pkl\")),\n","        \"MFO\" : pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_{   NUM_LABELS_dict['MFO']}_MFO.pkl\")),\n","        \"BPO\" : pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_{   NUM_LABELS_dict['BPO']}_BPO.pkl\")),\n","    }\n","if TRAIN:\n","    \n","   \n","    for fold,(train_indice, test_indice) in enumerate(kfold.split(range(len(X)))):\n","        for aspect in [  \"BPO\", \"MFO\",\"CCO\",]:\n","        \n","\n","            y= ys[aspect]\n","\n","            #X=np.concatenate([X_train,X_validate])\n","            #y=pd.concat([y_train,y_validate])\n","\n","            num_classes=y.shape[1]\n","            print(f\" Num classes {num_classes}\")\n","\n","            # model = CNN1D( num_classes)\n","            model =MultiLayerPerceptron(  num_classes)\n","            # Define the optimizer\n","            #optimizer=tf.keras.optimizers.Adam(learning_rate=CustomSchedule(LEARNING_RATE, WARMUP_STEPS), beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n","            optimizer = tf.keras.optimizers.Adam()\n","\n","            # Define the loss function\n","            loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n","            model.compile(optimizer=optimizer,loss=loss_fn,metrics=['binary_accuracy', tf.keras.metrics.AUC()])\n","\n","\n","            \n","            checkpoint_path = PATHS[\"CHECKPOINT\"][config.env].format(aspect, fold) + '/modelo-{epoch:02d}'\n","            checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n","                filepath=checkpoint_path,\n","                save_weights_only=False,\n","                save_freq='epoch',\n","\n","            )\n","\n","\n","\n","            model.fit(x=X[train_indice],y=y.iloc[train_indice],epochs=12,batch_size=32, callbacks=[checkpoint_callback,PredictionFnCallback(X[test_indice],y.iloc[test_indice])],)\n","            #models.append(model)\n","            del model\n","\n","\n","        if VALIDATE:\n","            print(\"Validating ...\")\n","            predictions_t=None\n","  \n","            for aspect in [  \"BPO\", \"MFO\",\"CCO\",]:\n","                y= ys[aspect]\n","\n","                loaded_model = tf.keras.models.load_model( PATHS[\"CHECKPOINT\"][config.env].format(aspect,fold)+f'/modelo-{EPOCH_LOAD}')\n","                \n","                # Use the loaded model for predictions or further operations\n","                predictions = loaded_model.predict(X[test_indice])\n","\n","                custom_f1_score(y.iloc[test_indice],np.round(predictions,3) )\n","\n","                print(np.round(predictions,3)[:10])\n","                predictions=np.round(predictions,3)\n","                predictions=pd.DataFrame(predictions,index=y.iloc[test_indice].index,columns=y.iloc[test_indice].columns,)\n","                predictions=predictions.stack()\n","                predictions=predictions[predictions>0]\n","                print(predictions.shape)\n","                predictions_t=pd.concat([predictions_t,predictions])\n","            predictions_t=predictions_t.sort_values(ascending=False).groupby(level=0).head(1500)\n","\n","            predictions_t.to_csv(PATHS[\"PREDICTIONS\"][config.env].format(fold),header=False, index=True, sep=\"\\t\")\n","            labels.loc[labels.EntryID.isin(y.iloc[test_indice].index)].to_csv(PATHS[\"LABELS\"][config.env].format(fold),sep=\"\\t\",index=False)"]},{"cell_type":"code","execution_count":40,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:42:15.481892Z","iopub.status.busy":"2023-07-13T16:42:15.481386Z","iopub.status.idle":"2023-07-13T16:42:15.488877Z","shell.execute_reply":"2023-07-13T16:42:15.487675Z","shell.execute_reply.started":"2023-07-13T16:42:15.481862Z"},"trusted":true},"outputs":[],"source":["def get_dataset( datatype, embeddings_source):\n","        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n","            embeds = np.load(f\"{PATH}\" + embeds_map[embeddings_source] + \"/\" + datatype + \"_embeddings.npy\")\n","            ids = np.load(f\"{PATH}\" + embeds_map[embeddings_source] + \"/\" + datatype + \"_ids.npy\")\n","        \n","        if embeddings_source == \"T5\":\n","            embeds = np.load(f\"{PATH}\"  + embeds_map[embeddings_source] + \"/\" + datatype + \"_embeds.npy\")\n","            ids = np.load(f\"{PATH}\"  + embeds_map[embeddings_source] + \"/\" + datatype + \"_ids.npy\")\n","            \n","\n","        if datatype==\"train\":\n","            df_labels=pd.read_pickle(PATH_LABELS)\n","            \n","            return embeds,ids,df_labels.loc[ids]\n","        return embeds,ids\n","        \n"," "]},{"cell_type":"code","execution_count":41,"metadata":{"execution":{"iopub.execute_input":"2023-07-13T16:42:15.491120Z","iopub.status.busy":"2023-07-13T16:42:15.490493Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["4434/4434 [==============================] - 8s 2ms/step\n","(141865, 2000)\n","[[0.535 0.487 0.352 ... 0.    0.    0.   ]\n"," [0.669 0.587 0.486 ... 0.    0.    0.   ]\n"," [0.691 0.606 0.413 ... 0.    0.    0.   ]\n"," ...\n"," [0.534 0.576 0.389 ... 0.    0.    0.   ]\n"," [0.654 0.659 0.606 ... 0.    0.    0.   ]\n"," [0.364 0.393 0.258 ... 0.    0.    0.   ]]\n","141865 2000\n","141865\n","4434/4434 [==============================] - 8s 2ms/step\n","(141865, 2000)\n","[[0.679 0.683 0.542 ... 0.    0.    0.   ]\n"," [0.733 0.761 0.645 ... 0.    0.    0.   ]\n"," [0.804 0.764 0.653 ... 0.    0.    0.   ]\n"," ...\n"," [0.622 0.541 0.433 ... 0.    0.    0.   ]\n"," [0.795 0.745 0.642 ... 0.    0.    0.   ]\n"," [0.431 0.491 0.414 ... 0.    0.    0.   ]]\n","141865 2000\n","141865\n","4434/4434 [==============================] - 7s 2ms/step\n","(141865, 2000)\n","[[0.614 0.585 0.323 ... 0.    0.    0.   ]\n"," [0.675 0.666 0.387 ... 0.    0.    0.   ]\n"," [0.717 0.705 0.403 ... 0.    0.    0.   ]\n"," ...\n"," [0.707 0.642 0.424 ... 0.    0.    0.   ]\n"," [0.84  0.803 0.707 ... 0.    0.    0.   ]\n"," [0.427 0.424 0.282 ... 0.    0.    0.   ]]\n","141865 2000\n","141865\n","4434/4434 [==============================] - 8s 2ms/step\n","(141865, 2000)\n","[[0.586 0.403 0.317 ... 0.    0.    0.   ]\n"," [0.698 0.524 0.485 ... 0.    0.    0.   ]\n"," [0.713 0.46  0.352 ... 0.    0.    0.   ]\n"," ...\n"," [0.621 0.551 0.339 ... 0.    0.    0.   ]\n"," [0.609 0.525 0.354 ... 0.    0.    0.   ]\n"," [0.443 0.508 0.3   ... 0.    0.    0.   ]]\n","141865 2000\n","141848\n","4434/4434 [==============================] - 7s 2ms/step\n","(141865, 2000)\n","[[0.682 0.596 0.463 ... 0.    0.    0.   ]\n"," [0.733 0.646 0.52  ... 0.    0.    0.   ]\n"," [0.784 0.703 0.525 ... 0.    0.    0.   ]\n"," ...\n"," [0.706 0.681 0.492 ... 0.    0.    0.   ]\n"," [0.791 0.692 0.649 ... 0.    0.    0.   ]\n"," [0.885 0.791 0.652 ... 0.    0.    0.   ]]\n","141865 2000\n","141853\n","4434/4434 [==============================] - 7s 2ms/step\n","(141865, 1000)\n","[[0.631 0.588 0.06  ... 0.    0.    0.   ]\n"," [0.735 0.715 0.136 ... 0.    0.001 0.   ]\n"," [0.723 0.678 0.094 ... 0.    0.    0.   ]\n"," ...\n"," [0.413 0.238 0.653 ... 0.    0.    0.   ]\n"," [0.339 0.252 0.547 ... 0.    0.    0.   ]\n"," [0.47  0.527 0.11  ... 0.    0.    0.   ]]\n","141865 1000\n","141673\n","4434/4434 [==============================] - 7s 2ms/step\n","(141865, 1000)\n","[[0.735 0.709 0.022 ... 0.    0.001 0.   ]\n"," [0.754 0.736 0.045 ... 0.    0.003 0.   ]\n"," [0.716 0.709 0.043 ... 0.    0.001 0.   ]\n"," ...\n"," [0.476 0.293 0.84  ... 0.    0.    0.   ]\n"," [0.345 0.225 0.802 ... 0.    0.    0.   ]\n"," [0.454 0.5   0.099 ... 0.    0.    0.   ]]\n","141865 1000\n","141381\n","4434/4434 [==============================] - 7s 2ms/step\n","(141865, 1000)\n","[[0.801 0.767 0.028 ... 0.    0.002 0.   ]\n"," [0.829 0.793 0.055 ... 0.    0.002 0.   ]\n"," [0.796 0.766 0.098 ... 0.    0.001 0.   ]\n"," ...\n"," [0.259 0.231 0.474 ... 0.    0.    0.   ]\n"," [0.24  0.218 0.425 ... 0.    0.    0.   ]\n"," [0.516 0.55  0.043 ... 0.    0.    0.   ]]\n","141865 1000\n","141574\n","4434/4434 [==============================] - 8s 2ms/step\n","(141865, 1000)\n","[[0.702 0.688 0.024 ... 0.    0.004 0.   ]\n"," [0.729 0.697 0.038 ... 0.    0.01  0.   ]\n"," [0.713 0.702 0.046 ... 0.    0.002 0.   ]\n"," ...\n"," [0.421 0.287 0.498 ... 0.    0.    0.   ]\n"," [0.421 0.323 0.572 ... 0.    0.    0.   ]\n"," [0.483 0.528 0.228 ... 0.    0.    0.   ]]\n","141865 1000\n","141581\n","4434/4434 [==============================] - 8s 2ms/step\n","(141865, 1000)\n","[[0.76  0.696 0.012 ... 0.    0.    0.   ]\n"," [0.825 0.768 0.021 ... 0.    0.    0.   ]\n"," [0.821 0.768 0.026 ... 0.    0.    0.   ]\n"," ...\n"," [0.378 0.311 0.632 ... 0.    0.    0.   ]\n"," [0.322 0.32  0.548 ... 0.    0.    0.   ]\n"," [0.72  0.722 0.028 ... 0.    0.    0.   ]]\n","141865 1000\n","141452\n","4434/4434 [==============================] - 8s 2ms/step\n","(141865, 2000)\n","[[0.466 0.392 0.391 ... 0.    0.016 0.004]\n"," [0.596 0.528 0.528 ... 0.    0.036 0.013]\n"," [0.46  0.282 0.266 ... 0.    0.006 0.002]\n"," ...\n"," [0.628 0.273 0.195 ... 0.011 0.    0.003]\n"," [0.574 0.316 0.228 ... 0.007 0.    0.004]\n"," [0.436 0.308 0.307 ... 0.    0.    0.007]]\n","141865 2000\n","139894\n","4434/4434 [==============================] - 7s 2ms/step\n","(141865, 2000)\n","[[0.448 0.426 0.417 ... 0.    0.    0.   ]\n"," [0.607 0.459 0.447 ... 0.    0.001 0.   ]\n"," [0.546 0.302 0.288 ... 0.    0.001 0.   ]\n"," ...\n"," [0.683 0.225 0.15  ... 0.027 0.    0.001]\n"," [0.526 0.229 0.168 ... 0.022 0.    0.003]\n"," [0.504 0.408 0.414 ... 0.    0.    0.001]]\n","141865 2000\n","139310\n","4434/4434 [==============================] - 7s 2ms/step\n","(141865, 2000)\n","[[0.388 0.59  0.603 ... 0.    0.001 0.001]\n"," [0.495 0.579 0.587 ... 0.    0.001 0.001]\n"," [0.45  0.415 0.428 ... 0.    0.    0.   ]\n"," ...\n"," [0.648 0.385 0.299 ... 0.006 0.    0.002]\n"," [0.63  0.39  0.312 ... 0.004 0.    0.002]\n"," [0.412 0.403 0.405 ... 0.    0.    0.   ]]\n","141865 2000\n","137370\n","4434/4434 [==============================] - 6s 1ms/step\n","(141865, 2000)\n","[[0.478 0.426 0.439 ... 0.    0.001 0.001]\n"," [0.605 0.48  0.485 ... 0.    0.001 0.002]\n"," [0.557 0.401 0.414 ... 0.    0.    0.001]\n"," ...\n"," [0.606 0.317 0.269 ... 0.016 0.    0.001]\n"," [0.513 0.305 0.277 ... 0.003 0.001 0.003]\n"," [0.399 0.363 0.363 ... 0.    0.    0.002]]\n","141865 2000\n","141105\n","4434/4434 [==============================] - 8s 2ms/step\n","(141865, 2000)\n","[[0.542 0.534 0.539 ... 0.    0.001 0.002]\n"," [0.693 0.652 0.653 ... 0.    0.002 0.009]\n"," [0.635 0.532 0.534 ... 0.    0.001 0.001]\n"," ...\n"," [0.662 0.419 0.343 ... 0.002 0.    0.003]\n"," [0.636 0.386 0.329 ... 0.021 0.001 0.007]\n"," [0.44  0.473 0.465 ... 0.    0.    0.001]]\n","141865 2000\n","139569\n","(709325000,)\n","(266227813,)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Q9CQV8</td>\n","      <td>GO:0110165</td>\n","      <td>0.6192</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Q9CQV8</td>\n","      <td>GO:0005622</td>\n","      <td>0.5508</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Q9CQV8</td>\n","      <td>GO:0043226</td>\n","      <td>0.3994</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Q9CQV8</td>\n","      <td>GO:0043229</td>\n","      <td>0.3684</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Q9CQV8</td>\n","      <td>GO:0043227</td>\n","      <td>0.3144</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>266227808</th>\n","      <td>A0A3G2FQK2</td>\n","      <td>GO:0031102</td>\n","      <td>0.0004</td>\n","    </tr>\n","    <tr>\n","      <th>266227809</th>\n","      <td>A0A3G2FQK2</td>\n","      <td>GO:0042048</td>\n","      <td>0.0002</td>\n","    </tr>\n","    <tr>\n","      <th>266227810</th>\n","      <td>A0A3G2FQK2</td>\n","      <td>GO:0031670</td>\n","      <td>0.0002</td>\n","    </tr>\n","    <tr>\n","      <th>266227811</th>\n","      <td>A0A3G2FQK2</td>\n","      <td>GO:0002821</td>\n","      <td>0.0002</td>\n","    </tr>\n","    <tr>\n","      <th>266227812</th>\n","      <td>A0A3G2FQK2</td>\n","      <td>GO:0140253</td>\n","      <td>0.0004</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>266227813 rows × 3 columns</p>\n","</div>"],"text/plain":["                    0           1       2\n","0              Q9CQV8  GO:0110165  0.6192\n","1              Q9CQV8  GO:0005622  0.5508\n","2              Q9CQV8  GO:0043226  0.3994\n","3              Q9CQV8  GO:0043229  0.3684\n","4              Q9CQV8  GO:0043227  0.3144\n","...               ...         ...     ...\n","266227808  A0A3G2FQK2  GO:0031102  0.0004\n","266227809  A0A3G2FQK2  GO:0042048  0.0002\n","266227810  A0A3G2FQK2  GO:0031670  0.0002\n","266227811  A0A3G2FQK2  GO:0002821  0.0002\n","266227812  A0A3G2FQK2  GO:0140253  0.0004\n","\n","[266227813 rows x 3 columns]"]},"metadata":{},"output_type":"display_data"}],"source":["EPOCH_LOAD = \"12\"\n","\n","import gc \n","\n","\n","X_test, ids_test= get_dataset( \"test\", embeddings_source)\n","\n","predictions= None\n","preds= []\n","for aspect in [ \"CCO\", \"MFO\",\"BPO\",]:\n","    \n","    y= ys[aspect]\n","    predictions_test_df_t =None\n","    for fold in range(n_splits):\n","        if TRAIN:\n","            loaded_model = tf.keras.models.load_model( PATHS[\"CHECKPOINT\"][config.env].format(aspect,fold)+f'/modelo-{EPOCH_LOAD}')\n","        else:\n","            loaded_model = tf.keras.models.load_model( f'../results/preds_bp_aspect_kaggle/models_{aspect}_{fold}/modelo-{EPOCH_LOAD}')\n","        # Use the loaded model for predictions or further operations\n","\n","        predictions_test = np.round(loaded_model.predict(X_test),3)\n","        del loaded_model\n","        gc.collect()\n","        print(predictions_test.shape)\n","        print(predictions_test[:10])\n","\n","\n","        print(len(ids_test),len(y.columns))\n","        predictions_test=pd.DataFrame(predictions_test,index=ids_test,columns=y.columns,)\n","        print((predictions_test==0).sum().max())\n","        \n","        \n","        \n","\n","        predictions_test=predictions_test.stack()\n","        #predictions_test=predictions_test[predictions_test>0]\n","\n","        if predictions_test_df_t is None:\n","            predictions_test_df_t = predictions_test\n","        else:\n","            predictions_test_df_t = predictions_test_df_t+ predictions_test\n","        del predictions_test\n","        gc .collect()\n","                \n","    predictions_test_df_t=predictions_test_df_t/n_splits\n","    preds.append(predictions_test_df_t)\n","predictions=pd.concat(preds)\n","    \n","        \n","\n","                \n","\n","#predictions= predictions.sort_values(ascending=False).groupby(level=0).head(1500)    \n","\n","\n","        \n","        \n","\n","            \n","\n","print(predictions.shape)\n","\n","predictions=predictions [predictions >0]\n","print(predictions.shape)\n","\n","predictions.to_csv(\"submission.tsv\",header=False, index=True, sep=\"\\t\")\n","\n","\n","df_submission = pd.read_csv(\"submission.tsv\",header=None, sep=\"\\t\")\n","display(df_submission)\n","                            "]},{"cell_type":"code","execution_count":42,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","df_submission = pd.read_csv(\"submission.tsv\",header=None, sep=\"\\t\")"]},{"cell_type":"code","execution_count":43,"metadata":{},"outputs":[{"data":{"text/plain":["3791"]},"execution_count":43,"metadata":{},"output_type":"execute_result"}],"source":["df_submission .groupby(0).size().max()"]},{"cell_type":"code","execution_count":44,"metadata":{},"outputs":[{"data":{"text/plain":["'2.13.0'"]},"execution_count":44,"metadata":{},"output_type":"execute_result"}],"source":["tf.__version__"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":4}
