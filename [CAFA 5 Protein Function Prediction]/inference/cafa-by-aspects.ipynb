{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdd3e065",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:40.560785Z",
     "iopub.status.busy": "2023-07-26T14:48:40.560402Z",
     "iopub.status.idle": "2023-07-26T14:48:49.159225Z",
     "shell.execute_reply": "2023-07-26T14:48:49.158359Z"
    },
    "papermill": {
     "duration": 8.610857,
     "end_time": "2023-07-26T14:48:49.161744",
     "exception": false,
     "start_time": "2023-07-26T14:48:40.550887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n",
      "  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n",
      "/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\n",
      "caused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n",
      "  warnings.warn(f\"file system plugins are not loaded: {e}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is not using GPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "print(pd.__version__)\n",
    "import numpy as np\n",
    "\n",
    "# UTILITARIES\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "import tensorflow as tf\n",
    "import os\n",
    "\n",
    "# Check if GPU is available and TensorFlow is using it\n",
    "if tf.test.is_gpu_available():\n",
    "    print('TensorFlow is using GPU')\n",
    "else:\n",
    "    print('TensorFlow is not using GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3385f74f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:49.179666Z",
     "iopub.status.busy": "2023-07-26T14:48:49.178412Z",
     "iopub.status.idle": "2023-07-26T14:48:49.195949Z",
     "shell.execute_reply": "2023-07-26T14:48:49.194801Z"
    },
    "papermill": {
     "duration": 0.029304,
     "end_time": "2023-07-26T14:48:49.198675",
     "exception": false,
     "start_time": "2023-07-26T14:48:49.169371",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "env_=\"kaggle\"\n",
    "PATH = \"/kaggle/input/\"\n",
    "#PATH=\"data/\"\n",
    "if env_==\"COLAB_INIT\":\n",
    "  from google.colab import drive\n",
    "  drive.mount('/content/drive')\n",
    "  ! pip install kaggle\n",
    "  ! mkdir ~/.kaggle\n",
    "  os.getcwd()\n",
    "  ! pip install Bio\n",
    "\n",
    "\n",
    "MAIN_DIR = f\"{PATH}cafa-5-protein-function-prediction\"\n",
    "\n",
    "class config:\n",
    "    train_sequences_path = MAIN_DIR  + \"/Train/train_sequences.fasta\"\n",
    "    train_labels_path = MAIN_DIR + \"/Train/train_terms.tsv\"\n",
    "    test_sequences_path = MAIN_DIR + \"/Test (Targets)/testsuperset.fasta\"\n",
    "    \n",
    "    num_labels = 1500\n",
    "    n_epochs = 5\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "\n",
    "    env = env_\n",
    "\n",
    "    \n",
    "TRAIN =False\n",
    "embeddings_source= \"T5\"\n",
    "\n",
    "PATH_DATAFRAMES=PATH_LABELS=\"proteins/proteins\"\n",
    "PATH_DATAFRAMES_2D=\"data/data_train_embeds_2d\"\n",
    "if env_==\"kaggle\":\n",
    "    LABELS_FILE_SUFFIX = \"_count\"\n",
    "    PATH_DATAFRAMES=\"/kaggle/input/proteins/proteins\"\n",
    "    PATH_DATAFRAMES_2D=\"/kaggle/input/dataframes-train-cafa/data_train_embeds_2d\"\n",
    "    PATH_LABELS=\"/kaggle/input/labels-count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc9a77e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:49.216111Z",
     "iopub.status.busy": "2023-07-26T14:48:49.215392Z",
     "iopub.status.idle": "2023-07-26T14:48:49.534311Z",
     "shell.execute_reply": "2023-07-26T14:48:49.533132Z"
    },
    "papermill": {
     "duration": 0.330891,
     "end_time": "2023-07-26T14:48:49.537078",
     "exception": false,
     "start_time": "2023-07-26T14:48:49.206187",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>The Protein ID</th>\n",
       "      <th>The Gene Ontology term (GO) ID</th>\n",
       "      <th>Predicted link probability that GO appear in Protein</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A0A0A0MRZ7</td>\n",
       "      <td>GO:0000001</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A0A0A0MRZ7</td>\n",
       "      <td>GO:0000002</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A0A0A0MRZ8</td>\n",
       "      <td>GO:0000001</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A0A0A0MRZ8</td>\n",
       "      <td>GO:0000002</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A0A0A0MRZ9</td>\n",
       "      <td>GO:0000001</td>\n",
       "      <td>0.123</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  The Protein ID The Gene Ontology term (GO) ID  \\\n",
       "0     A0A0A0MRZ7                     GO:0000001   \n",
       "1     A0A0A0MRZ7                     GO:0000002   \n",
       "2     A0A0A0MRZ8                     GO:0000001   \n",
       "3     A0A0A0MRZ8                     GO:0000002   \n",
       "4     A0A0A0MRZ9                     GO:0000001   \n",
       "\n",
       "   Predicted link probability that GO appear in Protein  \n",
       "0                                              0.123     \n",
       "1                                              0.123     \n",
       "2                                              0.123     \n",
       "3                                              0.123     \n",
       "4                                              0.123     "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "sub = pd.read_csv(f\"{PATH}cafa-5-protein-function-prediction/sample_submission.tsv\", sep= \"\\t\", header = None)\n",
    "sub.columns = [\"The Protein ID\", \"The Gene Ontology term (GO) ID\", \"Predicted link probability that GO appear in Protein\"]\n",
    "sub.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c8b9f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:49.555379Z",
     "iopub.status.busy": "2023-07-26T14:48:49.554422Z",
     "iopub.status.idle": "2023-07-26T14:48:49.560597Z",
     "shell.execute_reply": "2023-07-26T14:48:49.559522Z"
    },
    "papermill": {
     "duration": 0.018428,
     "end_time": "2023-07-26T14:48:49.563528",
     "exception": false,
     "start_time": "2023-07-26T14:48:49.545100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "PATHS = {\n",
    "    \"PATH\": PATH, \n",
    "    \"PATH_LABELS\": PATH_LABELS,\n",
    "    \"PATH_DATAFRAMES_2D\": PATH_DATAFRAMES_2D,\n",
    "    \"CHECKPOINT\": {\"local\":'../results/models_folds_aspect/models_{}_{}', \"kaggle\": '/kaggle/working/models_{}_{}'},\n",
    "    \"PREDICTIONS\": {\"local\":'../results/preds_cafa_embeddings_bp/preds_fold{}.tsv', \"kaggle\": 'preds_fold{}.tsv'},\n",
    "    \"LABELS\": {\"local\":'../results/preds_cafa_embeddings_bp_aspect/labels_used_fold{}.tsv', \"kaggle\": 'labels_used_fold{}.tsv'},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "196e8000",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:49.581343Z",
     "iopub.status.busy": "2023-07-26T14:48:49.580248Z",
     "iopub.status.idle": "2023-07-26T14:48:57.293246Z",
     "shell.execute_reply": "2023-07-26T14:48:57.292057Z"
    },
    "papermill": {
     "duration": 7.724406,
     "end_time": "2023-07-26T14:48:57.295673",
     "exception": false,
     "start_time": "2023-07-26T14:48:49.571267",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading train set ProtBERT Embeddings...\n",
      "Total Nb of Elements :  142246\n",
      "Total Nb of Elements :  141865\n"
     ]
    }
   ],
   "source": [
    "from Bio import SeqIO\n",
    "print(\"Loading train set ProtBERT Embeddings...\")\n",
    "fasta_train = SeqIO.parse(config.train_sequences_path, \"fasta\")\n",
    "print(\"Total Nb of Elements : \", len(list(fasta_train)))\n",
    "fasta_test= SeqIO.parse(config.test_sequences_path, \"fasta\")\n",
    "print(\"Total Nb of Elements : \", len(list(fasta_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d613133",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:57.315304Z",
     "iopub.status.busy": "2023-07-26T14:48:57.314862Z",
     "iopub.status.idle": "2023-07-26T14:48:57.321575Z",
     "shell.execute_reply": "2023-07-26T14:48:57.320377Z"
    },
    "papermill": {
     "duration": 0.018556,
     "end_time": "2023-07-26T14:48:57.323886",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.305330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if TRAIN:\n",
    "     weights=pd.read_csv(f\"{PATH}cafa-5-protein-function-prediction/IA.txt\",sep=\"\\t\",header=None,index_col=0).rename(columns={1:\"weight\"})\n",
    "     \n",
    "     labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "     labels.shape\n",
    "     labels= labels.loc[~labels.term.isin(list(weights[weights.weight==0].index))]\n",
    "     labels.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b7dedd",
   "metadata": {
    "papermill": {
     "duration": 0.008084,
     "end_time": "2023-07-26T14:48:57.339913",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.331829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb4df5b",
   "metadata": {
    "papermill": {
     "duration": 0.007821,
     "end_time": "2023-07-26T14:48:57.355592",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.347771",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85908d02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:57.373482Z",
     "iopub.status.busy": "2023-07-26T14:48:57.373058Z",
     "iopub.status.idle": "2023-07-26T14:48:57.378387Z",
     "shell.execute_reply": "2023-07-26T14:48:57.377313Z"
    },
    "papermill": {
     "duration": 0.017075,
     "end_time": "2023-07-26T14:48:57.380743",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.363668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Directories for the different embedding vectors : \n",
    "embeds_map = {\n",
    "    \"T5\" : \"my-t5embeds\",\n",
    "    \"ProtBERT\" : \"protbert-embeddings-for-cafa5\",\n",
    "    \"EMS2\" : \"cafa-5-ems-2-embeddings-numpy\"\n",
    "}\n",
    "\n",
    "# Length of the different embedding vectors :\n",
    "embeds_dim = {\n",
    "    \"T5\" : 1024,\n",
    "    \"ProtBERT\" : 1024,\n",
    "    \"EMS2\" : 1280\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14562e63",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:57.399446Z",
     "iopub.status.busy": "2023-07-26T14:48:57.398108Z",
     "iopub.status.idle": "2023-07-26T14:48:57.410576Z",
     "shell.execute_reply": "2023-07-26T14:48:57.409341Z"
    },
    "papermill": {
     "duration": 0.024579,
     "end_time": "2023-07-26T14:48:57.413302",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.388723",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPredictionFnCallback is used for:\\n1. Loading validation data\\n2. FOGModel data preparation\\n3. Prediction\\n4. Scoring and save\\n\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " LEARNING_RATE=0.001\n",
    " WARMUP_STEPS=7\n",
    " class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, initial_lr, warmup_steps=1):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.initial_lr = tf.cast(initial_lr, tf.float32)\n",
    "        self.warmup_steps = tf.cast(warmup_steps, tf.float32)\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        return tf.math.minimum(self.initial_lr, self.initial_lr * (step/self.warmup_steps))  \n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"initial_learning_rate\": self.initial_lr}\n",
    "    \n",
    "\n",
    "'''\n",
    "PredictionFnCallback is used for:\n",
    "1. Loading validation data\n",
    "2. FOGModel data preparation\n",
    "3. Prediction\n",
    "4. Scoring and save\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f76134ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:57.431592Z",
     "iopub.status.busy": "2023-07-26T14:48:57.431149Z",
     "iopub.status.idle": "2023-07-26T14:48:57.446768Z",
     "shell.execute_reply": "2023-07-26T14:48:57.445574Z"
    },
    "papermill": {
     "duration": 0.027825,
     "end_time": "2023-07-26T14:48:57.449350",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.421525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# version 2 ,añadir bacth norm a la version anterior\n",
    "class ResidualBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(units)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.fc2 = tf.keras.layers.Dense(units)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "\n",
    "    def call(self, x):\n",
    "        residual = x\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = x + residual\n",
    "        return x\n",
    "\n",
    "class MultiLayerPerceptron(tf.keras.Model):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MultiLayerPerceptron, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(512)\n",
    "        self.bn1 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu1 = tf.keras.layers.ReLU()\n",
    "        self.fc2 = tf.keras.layers.Dense(256)\n",
    "        self.bn2 = tf.keras.layers.BatchNormalization()\n",
    "        self.relu2 = tf.keras.layers.ReLU()\n",
    "        self.res_block1 = ResidualBlock(256)\n",
    "        self.res_block2 = ResidualBlock(256)\n",
    "        self.fc3 = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.res_block1(x)\n",
    "        x = self.res_block2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a055d3c",
   "metadata": {
    "papermill": {
     "duration": 0.00789,
     "end_time": "2023-07-26T14:48:57.465760",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.457870",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7fad4501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:57.483750Z",
     "iopub.status.busy": "2023-07-26T14:48:57.483328Z",
     "iopub.status.idle": "2023-07-26T14:48:57.490078Z",
     "shell.execute_reply": "2023-07-26T14:48:57.488721Z"
    },
    "papermill": {
     "duration": 0.018504,
     "end_time": "2023-07-26T14:48:57.492356",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.473852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "if TRAIN:\n",
    "    #X_train = np.load(os.path.join(PATH_DATAFRAMES,f\"embeds_train_{embeddings_source}.npy\"))\n",
    "    #X_validate = np.load(os.path.join(PATH_DATAFRAMES,f\"embeds_test_{embeddings_source}.npy\"))\n",
    "    #y_train = pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_train_{   config. num_labels}.pkl\"))\n",
    "    #y_validate = pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_test_{   config. num_labels}.pkl\"))\n",
    "    X=np.load(os.path.join(PATH_DATAFRAMES,f\"embeds_{embeddings_source}.npy\")).astype(np. float32)\n",
    "\n",
    "    print(X.shape)\n",
    "\n",
    "    labels = pd.read_csv(config.train_labels_path, sep = \"\\t\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b230a1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:57.510942Z",
     "iopub.status.busy": "2023-07-26T14:48:57.510151Z",
     "iopub.status.idle": "2023-07-26T14:48:58.600958Z",
     "shell.execute_reply": "2023-07-26T14:48:58.600104Z"
    },
    "papermill": {
     "duration": 1.102837,
     "end_time": "2023-07-26T14:48:58.603477",
     "exception": false,
     "start_time": "2023-07-26T14:48:57.500640",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "n_splits = 5\n",
    "\n",
    "# Crear una instancia de KFold\n",
    "kfold = KFold(n_splits=n_splits)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f759d4c",
   "metadata": {
    "papermill": {
     "duration": 0.007848,
     "end_time": "2023-07-26T14:48:58.619785",
     "exception": false,
     "start_time": "2023-07-26T14:48:58.611937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a226684",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:58.638389Z",
     "iopub.status.busy": "2023-07-26T14:48:58.637952Z",
     "iopub.status.idle": "2023-07-26T14:48:58.679277Z",
     "shell.execute_reply": "2023-07-26T14:48:58.678160Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "papermill": {
     "duration": 0.054203,
     "end_time": "2023-07-26T14:48:58.682028",
     "exception": false,
     "start_time": "2023-07-26T14:48:58.627825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def compute_metrics_(tau_arr, g, pred, toi, n_gt, wn_gt=None, ic_arr=None):\n",
    "\n",
    "    metrics = np.zeros((len(tau_arr), 7), dtype='float')  # cov, pr, rc, wpr, wrc, ru, mi\n",
    "\n",
    "    for i, tau in enumerate(tau_arr):\n",
    "\n",
    "        p = solidify_prediction(pred[:, toi], tau)\n",
    "\n",
    "        # number of proteins with at least one term predicted with score >= tau\n",
    "        metrics[i, 0] = (p.sum(axis=1) > 0).sum()\n",
    "\n",
    "        # Terms subsets\n",
    "        intersection = np.logical_and(p, g)  # TP\n",
    "\n",
    "        \n",
    "\n",
    "        # Subsets size\n",
    "        n_pred = p.sum(axis=1)\n",
    "        n_intersection = intersection.sum(axis=1)\n",
    "\n",
    "        # Precision, recall\n",
    "        metrics[i, 1] = np.divide(n_intersection, n_pred, out=np.zeros_like(n_intersection, dtype='float'),\n",
    "                                  where=n_pred > 0).sum() \n",
    "        metrics[i, 2] = np.divide(n_intersection, n_gt, out=np.zeros_like(n_gt, dtype='float'), where=n_gt > 0).sum()\n",
    "\n",
    "        if ic_arr is not None:\n",
    "            # Terms subsets\n",
    "            remaining = np.logical_and(np.logical_not(p), g)  # FN --> not predicted but in the ground truth\n",
    "            mis = np.logical_and(p, np.logical_not(g))  # FP --> predicted but not in the ground truth\n",
    "\n",
    "            # Weighted precision, recall\n",
    "            #wn_pred = (p * ic_arr[toi]).sum(axis=1)\n",
    "            wn_pred=np.dot(p , ic_arr[toi])\n",
    "            #wn_intersection = (intersection * ic_arr[toi]).sum(axis=1)\n",
    "            wn_intersection = np.dot(intersection ,ic_arr[toi])\n",
    "            \n",
    "            #print(f\"Shape {wn_intersection.shape},{wn_pred.shape},  {n_intersection.shape},{n_pred.shape}\")\n",
    "            metrics[i, 3] = np.divide(wn_intersection.reshape(-1), wn_pred.reshape(-1), out=np.zeros_like(n_intersection, dtype='float'),\n",
    "                                      where=wn_pred.reshape(-1) > 0).sum() \n",
    "            metrics[i, 4] = np.divide(wn_intersection.reshape(-1), wn_gt.reshape(-1), out=np.zeros_like(n_intersection, dtype='float'),\n",
    "                                      where=wn_gt.reshape(-1) > 0).sum() \n",
    "\n",
    "            # Misinformation, remaining uncertainty\n",
    "            #metrics[i, 5] = (remaining * ic_arr[toi]).sum(axis=1).sum()\n",
    "            metrics[i, 5] = np.dot(remaining , ic_arr[toi]).sum()\n",
    "            #metrics[i, 6] = (mis * ic_arr[toi]).sum(axis=1).sum()\n",
    "            metrics[i, 6] = np.dot(mis , ic_arr[toi]).sum()\n",
    "    return metrics\n",
    "# computes the f metric for each precision and recall in the input arrays\n",
    "def compute_f(pr, rc):\n",
    "    n = 2 * pr * rc\n",
    "    d = pr + rc\n",
    "    return np.divide(n, d, out=np.zeros_like(n, dtype=float), where=d != 0)\n",
    "\n",
    "\n",
    "def compute_s(ru, mi):\n",
    "    return np.sqrt(ru**2 + mi**2)\n",
    "    # return np.where(np.isnan(ru), mi, np.sqrt(ru + np.nan_to_num(mi)))\n",
    "\n",
    "def compute_metrics(pred, gt, toi, tau_arr, ic_arr=None, n_cpu=0):\n",
    "    \"\"\"\n",
    "    Takes the prediction and the ground truth and for each threshold in tau_arr\n",
    "    calculates the confusion matrix and returns the coverage,\n",
    "    precision, recall, remaining uncertainty and misinformation.\n",
    "    Toi is the list of terms (indexes) to be considered\n",
    "    \"\"\"\n",
    "    g = gt[:, toi]\n",
    "    n_gt = g.sum(axis=1)\n",
    "    wn_gt = None\n",
    "    if ic_arr is not None:\n",
    "        #wn_gt = (g * ic_arr[toi]).sum(axis=1)\n",
    "        wn_gt =np.dot(g,ic_arr[toi])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    arg_lists = [[tau_arr, g, pred, toi, n_gt, wn_gt, ic_arr] for tau_arr in np.array_split(tau_arr, 1)]\n",
    "    #print(f\" Nº  de cpus {n_cpu}, nº de args {len(   arg_lists)}\")\n",
    "    #with mp.Pool(processes=n_cpu) as pool:\n",
    "    #    metrics = np.concatenate(pool.starmap(compute_metrics_, arg_lists), axis=0)\n",
    "\n",
    "    results = []\n",
    "    for args in arg_lists:\n",
    "        result = compute_metrics_(*args)\n",
    "        results.append(result)\n",
    "\n",
    "    metrics = np.concatenate(results, axis=0)\n",
    "    metrics=pd.DataFrame(metrics, columns=[\"cov\", \"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"])\n",
    "    for column in [\"pr\", \"rc\", \"wpr\", \"wrc\", \"ru\", \"mi\"]:\n",
    "        metrics[column] = np.divide(metrics[column], metrics[\"cov\"], out=np.zeros_like(metrics[column], dtype='float'), where=metrics[\"cov\"] > 0)\n",
    "\n",
    "    ne = np.full(len(tau_arr), gt.shape[0])\n",
    "    metrics['ns'] = [\"\"] * len(tau_arr)\n",
    "    metrics['tau'] = tau_arr\n",
    "    metrics['cov'] = np.divide(metrics['cov'], ne, out=np.zeros_like(metrics['cov'], dtype='float'), where=ne > 0)\n",
    "    metrics['f'] = compute_f(metrics['pr'], metrics['rc'])\n",
    "    metrics['wf'] = compute_f(metrics['wpr'], metrics['wrc'])\n",
    "    metrics['s'] = compute_s(metrics['ru'], metrics['mi'])\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Return a mask for all the predictions (matrix) >= tau\n",
    "def solidify_prediction(pred, tau):\n",
    "    return pred >= tau\n",
    "def custom_f1_score(y_validate,y_pred):\n",
    "            max_scores={}\n",
    "            max_umbrales={}\n",
    "            scores_dict={}\n",
    "            for aspect,indice in aspects.items():\n",
    "                scores_dict[aspect]={}\n",
    "                max_score=0\n",
    "                max_umbral=0\n",
    "                for umbral in np.arange(0.05,0.5,0.05):\n",
    "\n",
    "                    y_pred_=(y_pred>umbral).astype(int)[:,indice]\n",
    "                    scores_sums=y_pred_.sum(axis=1)\n",
    "            \n",
    "                    n=(scores_sums>0.5).sum()\n",
    "                    #print(f\"n = {n} Secuencias con algun score 1 {n/len(y_pred_)}\")\n",
    "                    y_validate_=y_validate.values[:,indice]\n",
    "\n",
    "                    y_validate_sums=y_validate_.sum(axis=1)\n",
    "                    w_use_=w_use[indice,:]\n",
    "\n",
    "                    f1_scores = f1_score(y_validate_, y_pred_, average=None)\n",
    "                    \n",
    "                    t_p=y_pred_*y_validate_\n",
    "                    \n",
    "                    numerator=(np.dot(t_p,w_use_))\n",
    "                    precision_denom=np.dot(y_pred_,w_use_)\n",
    "                    precision_denom[precision_denom==0]=1\n",
    "                    weighted_precision= np.sum(   numerator /  precision_denom) / n\n",
    "\n",
    "\n",
    "                    \n",
    "                    recall_denom= np.dot(y_validate_,w_use_)\n",
    "                    recall_denom[recall_denom==0]=1\n",
    "\n",
    "                \n",
    "                    weighted_recall=    np.sum(numerator /   recall_denom) / len(y_pred_)\n",
    "                    average_f1_score = 2*weighted_precision* weighted_recall/( weighted_recall+ weighted_precision)\n",
    "            \n",
    "                    scores_dict[aspect][umbral]= average_f1_score \n",
    "                # Calcula el F1-score promedio\n",
    "                #average_f1_score = np.mean(f1_scores)\n",
    "                    if  average_f1_score > max_score:\n",
    "                        max_score=average_f1_score \n",
    "                        max_umbral=umbral\n",
    "                max_scores[aspect]= max_score\n",
    "                max_umbrales[aspect]=umbral\n",
    "            \n",
    "            max_score=np.mean(list(max_scores.values()))\n",
    "            print(f\"Scores {scores_dict}\")\n",
    "            # Print the F1 score\n",
    "            print('F1 Score:', max_score)\n",
    "\n",
    "class PredictionFnCallback(tf.keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self,X_validate,y_validate, model=None, verbose=0):\n",
    "        \n",
    "               if not model is None: self.model = model\n",
    "               self.verbose = verbose\n",
    "               self.X_validate = X_validate\n",
    "               self.y_validate = y_validate\n",
    "               self.num_classes=num_classes\n",
    "            \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "       \n",
    "        y_pred=(self.model.predict(self.X_validate))\n",
    "        #custom_f1_score(y_validate,y_pred)\n",
    "        ms = []\n",
    "        w_use=weights.loc[self.y_validate.columns].values\n",
    "        ms_=(compute_metrics(y_pred, self.y_validate.values, list(range(self.y_validate.shape[1])), np.arange(0.1,0.5,0.05), ic_arr=w_use, n_cpu=0))\n",
    "        ms.append(ms_.wf.max())\n",
    "        print(np.mean(ms))\n",
    "            \n",
    "\n",
    "def custom_f1_score(y_validate,y_pred):\n",
    "        #custom_f1_score(y_validate,y_pred)\n",
    "        ms = []\n",
    "        w_use=weights.loc[y_validate.columns].values\n",
    "        ms_=(compute_metrics(y_pred,y_validate.values, list(range(y_validate.shape[1])), np.arange(0.1,0.5,0.05), ic_arr=w_use, n_cpu=0))\n",
    "        ms.append(ms_.wf.max())\n",
    "        print(np.mean(ms))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366d3ef5",
   "metadata": {
    "papermill": {
     "duration": 0.007788,
     "end_time": "2023-07-26T14:48:58.698181",
     "exception": false,
     "start_time": "2023-07-26T14:48:58.690393",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "384c7efa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:48:58.716756Z",
     "iopub.status.busy": "2023-07-26T14:48:58.715984Z",
     "iopub.status.idle": "2023-07-26T14:50:10.914115Z",
     "shell.execute_reply": "2023-07-26T14:50:10.913156Z"
    },
    "papermill": {
     "duration": 72.210609,
     "end_time": "2023-07-26T14:50:10.916971",
     "exception": false,
     "start_time": "2023-07-26T14:48:58.706362",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')\n",
    "models = []\n",
    "EPOCH_LOAD = \"12\"\n",
    "VALIDATE =True\n",
    "NUM_LABELS_dict= {        \"BPO\": 3000,\n",
    "                              \"CCO\":1000,\n",
    "                              \"MFO\": 2000}\n",
    "ys={\n",
    "        \"CCO\" : pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_{   NUM_LABELS_dict['CCO']}_CCO{LABELS_FILE_SUFFIX }.pkl\")),\n",
    "        \"MFO\" : pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_{   NUM_LABELS_dict['MFO']}_MFO{LABELS_FILE_SUFFIX }.pkl\")),\n",
    "        \"BPO\" : pd.read_pickle(os.path.join(PATH_LABELS,f\"labels_{   NUM_LABELS_dict['BPO']}_BPO{LABELS_FILE_SUFFIX }.pkl\")),\n",
    "    }\n",
    "if TRAIN:\n",
    "    \n",
    "   \n",
    "    for fold,(train_indice, test_indice) in enumerate(kfold.split(range(len(X)))):\n",
    "        for aspect in [  \"BPO\", \"MFO\",\"CCO\",]:\n",
    "            \n",
    "            y= ys[aspect]\n",
    "            w_use_=weights.loc[y.columns].values\n",
    "            w_loss=w_use_.reshape(-1)\n",
    "            w_loss_sum =np.sum(w_loss)\n",
    "            bce= tf.keras.losses.BinaryCrossentropy(from_logits=False,reduction=\"none\")\n",
    "            def weighted_binary_crossentropy(y_true, y_pred):\n",
    "                # Define the weights\n",
    "\n",
    "                # Adjust the weights as per your requirement\n",
    "\n",
    "                # Apply the weights to the binary cross entropy\n",
    "                bce_loss = bce(tf.expand_dims(y_true,-1), tf.expand_dims(y_pred,-1) )\n",
    "                print(bce_loss.shape,w_loss.shape)\n",
    "\n",
    "                weighted_bce = tf.multiply(bce_loss,w_loss )/w_loss_sum\n",
    "                #print(   tf.reduce_mean( weighted_bce ).shape)\n",
    "                return     tf.reduce_mean( weighted_bce )\n",
    "\n",
    "            tf.keras.utils.get_custom_objects()['weighted_binary_crossentropy'] = weighted_binary_crossentropy\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "            #X=np.concatenate([X_train,X_validate])\n",
    "            #y=pd.concat([y_train,y_validate])\n",
    "\n",
    "            num_classes=y.shape[1]\n",
    "            print(f\" Num classes {num_classes}\")\n",
    "\n",
    "            # model = CNN1D( num_classes)\n",
    "            model =MultiLayerPerceptron(  num_classes)\n",
    "            # Define the optimizer\n",
    "            #optimizer=tf.keras.optimizers.Adam(learning_rate=CustomSchedule(LEARNING_RATE, WARMUP_STEPS), beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "            optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "            # Define the loss function\n",
    "            loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "            model.compile(optimizer=optimizer,loss=  loss_fn, metrics=['binary_accuracy', tf.keras.metrics.AUC()])\n",
    "\n",
    "\n",
    "            \n",
    "            checkpoint_path = PATHS[\"CHECKPOINT\"][config.env].format(aspect, fold) + '/modelo-{epoch:02d}'\n",
    "            checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath=checkpoint_path,\n",
    "                save_weights_only=False,\n",
    "                save_freq='epoch',\n",
    "\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "            model.fit(x=X[train_indice],y=y.iloc[train_indice],epochs=12,batch_size=32, callbacks=[checkpoint_callback,PredictionFnCallback(X[test_indice],y.iloc[test_indice])],)\n",
    "            #models.append(model)\n",
    "            del model\n",
    "\n",
    "\n",
    "        if VALIDATE:\n",
    "            print(\"Validating ...\")\n",
    "            predictions_t=None\n",
    "  \n",
    "            for aspect in [  \"BPO\", \"MFO\",\"CCO\",]:\n",
    "                y= ys[aspect]\n",
    "\n",
    "                loaded_model = tf.keras.models.load_model( PATHS[\"CHECKPOINT\"][config.env].format(aspect,fold)+f'/modelo-{EPOCH_LOAD}')\n",
    "                \n",
    "                # Use the loaded model for predictions or further operations\n",
    "                predictions = loaded_model.predict(X[test_indice])\n",
    "\n",
    "                custom_f1_score(y.iloc[test_indice],np.round(predictions,3) )\n",
    "\n",
    "                print(np.round(predictions,3)[:10])\n",
    "                predictions=np.round(predictions,3)\n",
    "                predictions=pd.DataFrame(predictions,index=y.iloc[test_indice].index,columns=y.iloc[test_indice].columns,)\n",
    "                predictions=predictions.stack()\n",
    "                predictions=predictions[predictions>0]\n",
    "                print(predictions.shape)\n",
    "                predictions_t=pd.concat([predictions_t,predictions])\n",
    "            #predictions_t=predictions_t.sort_values(ascending=False).groupby(level=0).head(1500)\n",
    "\n",
    "            predictions_t.to_csv(PATHS[\"PREDICTIONS\"][config.env].format(fold),header=False, index=True, sep=\"\\t\")\n",
    "            labels.loc[labels.EntryID.isin(y.iloc[test_indice].index)].to_csv(PATHS[\"LABELS\"][config.env].format(fold),sep=\"\\t\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "340cb569",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:50:10.934657Z",
     "iopub.status.busy": "2023-07-26T14:50:10.934273Z",
     "iopub.status.idle": "2023-07-26T14:50:10.943119Z",
     "shell.execute_reply": "2023-07-26T14:50:10.942371Z"
    },
    "papermill": {
     "duration": 0.020088,
     "end_time": "2023-07-26T14:50:10.945294",
     "exception": false,
     "start_time": "2023-07-26T14:50:10.925206",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_dataset( datatype, embeddings_source):\n",
    "        if embeddings_source in [\"ProtBERT\", \"EMS2\"]:\n",
    "            embeds = np.load(f\"{PATH}\" + embeds_map[embeddings_source] + \"/\" + datatype + \"_embeddings.npy\")\n",
    "            ids = np.load(f\"{PATH}\" + embeds_map[embeddings_source] + \"/\" + datatype + \"_ids.npy\")\n",
    "        \n",
    "        if embeddings_source == \"T5\":\n",
    "            embeds = np.load(f\"{PATH}\"  + embeds_map[embeddings_source] + \"/\" + datatype + \"_embeds.npy\")\n",
    "            ids = np.load(f\"{PATH}\"  + embeds_map[embeddings_source] + \"/\" + datatype + \"_ids.npy\")\n",
    "            \n",
    "\n",
    "        if datatype==\"train\":\n",
    "            df_labels=pd.read_pickle(PATH_LABELS)\n",
    "            \n",
    "            return embeds,ids,df_labels.loc[ids]\n",
    "        return embeds,ids\n",
    "        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f45c7184",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-26T14:50:10.963386Z",
     "iopub.status.busy": "2023-07-26T14:50:10.962645Z",
     "iopub.status.idle": "2023-07-26T15:02:53.868485Z",
     "shell.execute_reply": "2023-07-26T15:02:53.867207Z"
    },
    "papermill": {
     "duration": 763.452787,
     "end_time": "2023-07-26T15:02:54.405839",
     "exception": false,
     "start_time": "2023-07-26T14:50:10.953052",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4434/4434 [==============================] - 19s 4ms/step\n",
      "(141865, 1000)\n",
      "[[0.    0.45  0.31  ... 0.    0.    0.   ]\n",
      " [0.    0.578 0.485 ... 0.    0.    0.   ]\n",
      " [0.    0.61  0.444 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.414 0.452 ... 0.    0.    0.   ]\n",
      " [0.    0.585 0.621 ... 0.    0.    0.   ]\n",
      " [0.    0.421 0.366 ... 0.    0.    0.001]]\n",
      "4434/4434 [==============================] - 18s 4ms/step\n",
      "(141865, 1000)\n",
      "[[0.    0.841 0.735 ... 0.    0.    0.   ]\n",
      " [0.    0.817 0.672 ... 0.    0.    0.   ]\n",
      " [0.    0.846 0.66  ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.361 0.264 ... 0.    0.    0.   ]\n",
      " [0.    0.74  0.612 ... 0.    0.    0.   ]\n",
      " [0.    0.324 0.376 ... 0.    0.    0.   ]]\n",
      "4434/4434 [==============================] - 18s 4ms/step\n",
      "(141865, 1000)\n",
      "[[0.    0.75  0.653 ... 0.    0.    0.   ]\n",
      " [0.    0.734 0.641 ... 0.    0.    0.   ]\n",
      " [0.    0.826 0.629 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.615 0.437 ... 0.    0.    0.   ]\n",
      " [0.    0.875 0.831 ... 0.    0.    0.   ]\n",
      " [0.    0.405 0.449 ... 0.    0.    0.   ]]\n",
      "4434/4434 [==============================] - 19s 4ms/step\n",
      "(141865, 1000)\n",
      "[[0.    0.777 0.661 ... 0.    0.    0.   ]\n",
      " [0.    0.866 0.791 ... 0.    0.    0.   ]\n",
      " [0.    0.819 0.736 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.566 0.55  ... 0.    0.    0.   ]\n",
      " [0.    0.591 0.604 ... 0.    0.    0.   ]\n",
      " [0.    0.776 0.692 ... 0.    0.    0.   ]]\n",
      "4434/4434 [==============================] - 18s 4ms/step\n",
      "(141865, 1000)\n",
      "[[0.    0.759 0.668 ... 0.    0.    0.   ]\n",
      " [0.    0.787 0.721 ... 0.    0.    0.   ]\n",
      " [0.    0.783 0.759 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.7   0.583 ... 0.    0.    0.   ]\n",
      " [0.    0.707 0.644 ... 0.    0.    0.   ]\n",
      " [0.    0.589 0.649 ... 0.    0.    0.   ]]\n",
      "141865 1000\n",
      "4434/4434 [==============================] - 20s 5ms/step\n",
      "(141865, 2000)\n",
      "[[0.    0.702 0.702 ... 0.    0.    0.   ]\n",
      " [0.    0.733 0.719 ... 0.    0.    0.   ]\n",
      " [0.    0.757 0.733 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.181 0.115 ... 0.    0.    0.   ]\n",
      " [0.    0.192 0.147 ... 0.    0.    0.   ]\n",
      " [0.    0.485 0.442 ... 0.    0.    0.   ]]\n",
      "4434/4434 [==============================] - 20s 5ms/step\n",
      "(141865, 2000)\n",
      "[[0.    0.853 0.814 ... 0.    0.    0.   ]\n",
      " [0.    0.894 0.893 ... 0.    0.    0.   ]\n",
      " [0.    0.858 0.829 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.304 0.234 ... 0.    0.002 0.   ]\n",
      " [0.    0.133 0.136 ... 0.    0.    0.   ]\n",
      " [0.    0.806 0.809 ... 0.    0.    0.   ]]\n",
      "4434/4434 [==============================] - 21s 5ms/step\n",
      "(141865, 2000)\n",
      "[[0.    0.789 0.796 ... 0.    0.    0.   ]\n",
      " [0.    0.837 0.834 ... 0.    0.    0.   ]\n",
      " [0.    0.831 0.836 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.296 0.227 ... 0.    0.001 0.   ]\n",
      " [0.    0.296 0.205 ... 0.    0.    0.   ]\n",
      " [0.    0.53  0.552 ... 0.    0.    0.   ]]\n",
      "4434/4434 [==============================] - 26s 6ms/step\n",
      "(141865, 2000)\n",
      "[[0.    0.846 0.814 ... 0.    0.    0.   ]\n",
      " [0.    0.805 0.789 ... 0.    0.    0.   ]\n",
      " [0.    0.87  0.822 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.293 0.245 ... 0.    0.003 0.   ]\n",
      " [0.    0.233 0.189 ... 0.    0.001 0.   ]\n",
      " [0.    0.495 0.476 ... 0.    0.    0.   ]]\n",
      "4434/4434 [==============================] - 21s 5ms/step\n",
      "(141865, 2000)\n",
      "[[0.    0.851 0.782 ... 0.    0.    0.   ]\n",
      " [0.    0.869 0.828 ... 0.    0.    0.   ]\n",
      " [0.    0.859 0.788 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.36  0.26  ... 0.    0.001 0.   ]\n",
      " [0.    0.377 0.346 ... 0.    0.    0.   ]\n",
      " [0.    0.679 0.726 ... 0.    0.    0.   ]]\n",
      "141865 2000\n",
      "4434/4434 [==============================] - 24s 5ms/step\n",
      "(141865, 3000)\n",
      "[[0.    0.561 0.487 ... 0.    0.    0.001]\n",
      " [0.    0.597 0.531 ... 0.    0.    0.   ]\n",
      " [0.    0.624 0.504 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.832 0.365 ... 0.    0.04  0.   ]\n",
      " [0.    0.457 0.256 ... 0.    0.    0.   ]\n",
      " [0.    0.518 0.48  ... 0.    0.    0.001]]\n",
      "4434/4434 [==============================] - 23s 5ms/step\n",
      "(141865, 3000)\n",
      "[[0.    0.382 0.532 ... 0.    0.    0.   ]\n",
      " [0.    0.508 0.623 ... 0.    0.    0.   ]\n",
      " [0.    0.525 0.487 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.633 0.363 ... 0.    0.012 0.   ]\n",
      " [0.    0.483 0.363 ... 0.    0.004 0.   ]\n",
      " [0.    0.383 0.472 ... 0.    0.    0.001]]\n",
      "4434/4434 [==============================] - 23s 5ms/step\n",
      "(141865, 3000)\n",
      "[[0.    0.742 0.693 ... 0.    0.    0.   ]\n",
      " [0.    0.701 0.571 ... 0.    0.    0.   ]\n",
      " [0.    0.738 0.571 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.73  0.329 ... 0.    0.021 0.   ]\n",
      " [0.    0.593 0.185 ... 0.003 0.011 0.   ]\n",
      " [0.    0.217 0.199 ... 0.    0.    0.   ]]\n",
      "4434/4434 [==============================] - 23s 5ms/step\n",
      "(141865, 3000)\n",
      "[[0.    0.64  0.657 ... 0.    0.    0.   ]\n",
      " [0.    0.609 0.67  ... 0.001 0.    0.   ]\n",
      " [0.    0.669 0.645 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.706 0.24  ... 0.001 0.005 0.   ]\n",
      " [0.    0.627 0.28  ... 0.001 0.003 0.   ]\n",
      " [0.    0.456 0.565 ... 0.    0.    0.001]]\n",
      "4434/4434 [==============================] - 23s 5ms/step\n",
      "(141865, 3000)\n",
      "[[0.    0.58  0.716 ... 0.    0.    0.   ]\n",
      " [0.    0.669 0.734 ... 0.    0.    0.   ]\n",
      " [0.    0.655 0.642 ... 0.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.79  0.552 ... 0.001 0.037 0.   ]\n",
      " [0.    0.473 0.158 ... 0.    0.003 0.   ]\n",
      " [0.    0.415 0.336 ... 0.    0.    0.   ]]\n",
      "141865 3000\n",
      "(47075001,)\n",
      "(47075001,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Q9CQV8</td>\n",
       "      <td>GO:0110165</td>\n",
       "      <td>0.7154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Q9CQV8</td>\n",
       "      <td>GO:0005622</td>\n",
       "      <td>0.6054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Q9CQV8</td>\n",
       "      <td>GO:0043226</td>\n",
       "      <td>0.4864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Q9CQV8</td>\n",
       "      <td>GO:0043229</td>\n",
       "      <td>0.4286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Q9CQV8</td>\n",
       "      <td>GO:0043227</td>\n",
       "      <td>0.4240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47074996</th>\n",
       "      <td>A0A3G2FQK2</td>\n",
       "      <td>GO:0019730</td>\n",
       "      <td>0.0280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47074997</th>\n",
       "      <td>A0A3G2FQK2</td>\n",
       "      <td>GO:1903317</td>\n",
       "      <td>0.0108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47074998</th>\n",
       "      <td>A0A3G2FQK2</td>\n",
       "      <td>GO:0050818</td>\n",
       "      <td>0.0158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47074999</th>\n",
       "      <td>A0A3G2FQK2</td>\n",
       "      <td>GO:0032642</td>\n",
       "      <td>0.0114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47075000</th>\n",
       "      <td>A0A3G2FQK2</td>\n",
       "      <td>GO:1900046</td>\n",
       "      <td>0.0166</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47075001 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   0           1       2\n",
       "0             Q9CQV8  GO:0110165  0.7154\n",
       "1             Q9CQV8  GO:0005622  0.6054\n",
       "2             Q9CQV8  GO:0043226  0.4864\n",
       "3             Q9CQV8  GO:0043229  0.4286\n",
       "4             Q9CQV8  GO:0043227  0.4240\n",
       "...              ...         ...     ...\n",
       "47074996  A0A3G2FQK2  GO:0019730  0.0280\n",
       "47074997  A0A3G2FQK2  GO:1903317  0.0108\n",
       "47074998  A0A3G2FQK2  GO:0050818  0.0158\n",
       "47074999  A0A3G2FQK2  GO:0032642  0.0114\n",
       "47075000  A0A3G2FQK2  GO:1900046  0.0166\n",
       "\n",
       "[47075001 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCH_LOAD = \"12\"\n",
    "\n",
    "import gc \n",
    "\n",
    "\n",
    "X_test, ids_test= get_dataset( \"test\", embeddings_source)\n",
    "\n",
    "predictions= None\n",
    "preds= []\n",
    "for aspect in [ \"CCO\", \"MFO\",\"BPO\",]:\n",
    "    \n",
    "    y= ys[aspect]\n",
    "    predictions_test_df_t =None\n",
    "    for fold in range(n_splits):\n",
    "        if TRAIN:\n",
    "            loaded_model = tf.keras.models.load_model( PATHS[\"CHECKPOINT\"][config.env].format(aspect,fold)+f'/modelo-{EPOCH_LOAD}')\n",
    "        else:\n",
    "            loaded_model = tf.keras.models.load_model( f'/kaggle/input/models-cafa/preds_aspect_count/models_{aspect}_{fold}/modelo-{EPOCH_LOAD}')\n",
    "        # Use the loaded model for predictions or further operations\n",
    "\n",
    "        predictions_test = np.round(loaded_model.predict(X_test),3)\n",
    "        del loaded_model\n",
    "        gc.collect()\n",
    "        print(predictions_test.shape)\n",
    "        print(predictions_test[:10])\n",
    "\n",
    "\n",
    "        \n",
    "        if predictions_test_df_t is None:\n",
    "            predictions_test_df_t = predictions_test\n",
    "        else:\n",
    "            predictions_test_df_t = predictions_test_df_t+ predictions_test\n",
    "        del predictions_test\n",
    "        gc .collect()\n",
    "                \n",
    "    predictions_test_df_t=predictions_test_df_t/n_splits\n",
    "    \n",
    "    print(len(ids_test),len(y.columns))\n",
    "    predictions_test_df_t=pd.DataFrame(    predictions_test_df_t,index=ids_test,columns=y.columns,)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    predictions_test_df_t=    predictions_test_df_t.stack()\n",
    "    predictions_test_df_t  =     predictions_test_df_t[    predictions_test_df_t>0.01]\n",
    "\n",
    "    if predictions is None:\n",
    "           predictions=predictions_test_df_t\n",
    "    else:\n",
    "        predictions=pd.concat(   [ predictions, predictions_test_df_t])\n",
    "        del predictions_test_df_t\n",
    "        gc.collect()\n",
    "    \n",
    "        \n",
    "\n",
    "                \n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "print(predictions.shape)\n",
    "\n",
    "predictions=predictions [predictions >0]\n",
    "print(predictions.shape)\n",
    "\n",
    "predictions.to_csv(\"submission.tsv\",header=False, index=True, sep=\"\\t\")\n",
    "\n",
    "\n",
    "df_submission = pd.read_csv(\"submission.tsv\",header=None, sep=\"\\t\")\n",
    "display(df_submission)\n",
    "                            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 870.600526,
   "end_time": "2023-07-26T15:02:58.315158",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-07-26T14:48:27.714632",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
